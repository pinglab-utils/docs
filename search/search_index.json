{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ICD 11 Code Tools This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Counting number of decendent nodes in each categories of ICD 11 code trees.","title":"Home"},{"location":"#icd-11-code-tools","text":"This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Counting number of decendent nodes in each categories of ICD 11 code trees.","title":"ICD 11 Code Tools"},{"location":"workflow/","text":"Cloud Articture and Workflow This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Platform architecture and workflow","title":"Workflow"},{"location":"workflow/#cloud-articture-and-workflow","text":"This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Platform architecture and workflow","title":"Cloud Articture and Workflow"},{"location":"algorithms/algI/Algorithms-I/","text":"Algorithms - I Distance, Centrality, Community and Traversal import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( \"ignore\" ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G) 1. Distance Graph diameter, radius, eccentricity and other properties. center(G[, e, usebounds]) Returns the center of the graph G. diameter(G[, e, usebounds]) Returns the diameter of the graph G. eccentricity(G[, v, sp]) Returns the eccentricity of nodes in G. extrema_bounding(G[, compute]) Compute requested extreme distance metric of undirected graph G periphery(G[, e, usebounds]) Returns the periphery of the graph G. radius(G[, e, usebounds]) Returns the radius of the graph G. nx . radius(G), nx . diameter(G) (3, 5) nx . center(G) [0, 1, 2, 3, 8, 13, 19, 31] nx . eccentricity(G) {0: 3, 1: 3, 2: 3, 3: 3, 4: 4, 5: 4, 6: 4, 7: 4, 8: 3, 9: 4, 10: 4, 11: 4, 12: 4, 13: 3, 14: 5, 15: 5, 16: 5, 17: 4, 18: 5, 19: 3, 20: 5, 21: 4, 22: 5, 23: 5, 24: 4, 25: 4, 26: 5, 27: 4, 28: 4, 29: 5, 30: 4, 31: 3, 32: 4, 33: 4} nx . extrema_bounding(G) 5 nx . periphery(G) [14, 15, 16, 18, 20, 22, 23, 26, 29] Centrality Degree degree_centrality(G) Compute the degree centrality for nodes. in_degree_centrality(G) Compute the in-degree centrality for nodes. out_degree_centrality(G) Compute the out-degree centrality for nodes. G = nx . balanced_tree(r = 3 , h = 2 ) nx . draw(G, node_size = 500 ,node_color = 'lightblue' , with_labels = True ) nx . degree_centrality(G) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . in_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . out_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} Eigenvector Centality eigenvector_centrality(G[, max_iter, tol, \u2026]) Compute the eigenvector centrality for the graph G. eigenvector_centrality_numpy(G[, weight, \u2026]) Compute the eigenvector centrality for the graph G. katz_centrality(G[, alpha, beta, max_iter, \u2026]) Compute the Katz centrality for the nodes of the graph G. katz_centrality_numpy(G[, alpha, beta, \u2026]) Compute the Katz centrality for the graph G. nx . eigenvector_centrality(G) {0: 0.49999972705204543, 1: 0.40824851115143324, 2: 0.40824851115143324, 3: 0.40824851115143324, 4: 0.16666657745857444, 5: 0.16666657745857444, 6: 0.16666657745857444, 7: 0.16666657745857444, 8: 0.16666657745857444, 9: 0.16666657745857444, 10: 0.16666657745857444, 11: 0.16666657745857444, 12: 0.16666657745857444} nx . katz_centrality(G) {0: 0.31855094383728, 1: 0.3279200842087277, 2: 0.3279200842087277, 3: 0.3279200842087277, 4: 0.2529669612371458, 5: 0.2529669612371458, 6: 0.2529669612371458, 7: 0.2529669612371458, 8: 0.2529669612371458, 9: 0.2529669612371458, 10: 0.2529669612371458, 11: 0.2529669612371458, 12: 0.2529669612371458} Betweenness Centrality betweenness_centrality(G[, k, normalized, \u2026]) Compute the shortest-path betweenness centrality for nodes. edge_betweenness_centrality(G[, k, \u2026]) Compute betweenness centrality for edges. betweenness_centrality_subset(G, sources, \u2026) Compute betweenness centrality for a subset of nodes. edge_betweenness_centrality_subset(G, \u2026[, \u2026]) Compute betweenness centrality for edges for a subset of nodes. nx . betweenness_centrality(G) {0: 0.7272727272727273, 1: 0.4545454545454546, 2: 0.4545454545454546, 3: 0.4545454545454546, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0} 2. Community Functions for computing and measuring community structure. The functions in this class are not imported into the top-level networkx namespace. You can access these functions by importing the networkx.algorithms.community module, then accessing the functions as attributes of community. For example: from networkx.algorithms import community G = nx . barbell_graph( 5 , 1 ) nx . draw(G) communities_generator = community . girvan_newman(G) top_level_communities = next (communities_generator) next_level_communities = next (communities_generator) sorted ( map ( sorted , next_level_communities)) [[0, 1, 2, 3, 4], [5], [6, 7, 8, 9, 10]] K-Clique Communty G = nx . relaxed_caveman_graph( 5 , 10 , 0.1 ) nx . draw(G, node_size = 40 ,node_color = 'blue' ) from networkx.algorithms.community import k_clique_communities c = list (k_clique_communities(G,k = 4 )) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import greedy_modularity_communities c = list (greedy_modularity_communities(G)) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import asyn_lpa_communities c = list (asyn_lpa_communities(G)) print (c) [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}] from networkx.algorithms.community import label_propagation_communities c = list (label_propagation_communities(G)) print (c) [{10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}] from networkx.algorithms.community import asyn_fluidc c = list (asyn_fluidc(G,k = 5 )) print (c) [{16, 17, 19, 11}, {10, 12, 13, 14, 15, 18}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 40, 9, 8, 43, 44, 45, 46, 47, 48, 49, 41, 42}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}] 3 Traversal 3.1 Depth First Search Basic algorithms for depth-first searching the nodes of a graph. dfs_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS). dfs_tree(G[, source, depth_limit]) Returns oriented tree constructed from a depth-first-search from source. dfs_predecessors(G[, source, depth_limit]) Returns dictionary of predecessors in depth-first-search from source. dfs_successors(G[, source, depth_limit]) Returns dictionary of successors in depth-first-search from source. dfs_preorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search pre-ordering starting at source. dfs_postorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search post-ordering starting at source. dfs_labeled_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS) labeled by type. G = nx . random_tree( 20 ) nx . draw(G, node_size = 200 ,node_color = 'lightblue' ,with_labels = True ) L = list (nx . dfs_edges(G, source = 0 , depth_limit = 5 )) print (L) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12)] TG = nx . dfs_tree(G) nx . draw(G, node_size = 200 ,node_color = 'lightblue' ,with_labels = True ) nx . dfs_predecessors(G, source = 0 ) {1: 19, 2: 12, 3: 11, 4: 18, 5: 13, 6: 8, 7: 5, 8: 4, 9: 12, 10: 1, 11: 2, 12: 10, 13: 16, 14: 0, 15: 5, 16: 18, 17: 3, 18: 11, 19: 14} nx . dfs_successors(G, source = 0 ) {0: [14], 1: [10], 2: [11], 3: [17], 4: [8], 5: [7, 15], 8: [6], 10: [12], 11: [3, 18], 12: [9, 2], 13: [5], 14: [19], 16: [13], 18: [4, 16], 19: [1]} list (nx . dfs_preorder_nodes(G)) [0, 14, 19, 1, 10, 12, 9, 2, 11, 3, 17, 18, 4, 8, 6, 16, 13, 5, 7, 15] list (nx . dfs_postorder_nodes(G)) [9, 17, 3, 6, 8, 4, 7, 15, 5, 13, 16, 18, 11, 2, 12, 10, 1, 19, 14, 0] list (nx . dfs_labeled_edges(G)) [(0, 0, 'forward'), (0, 14, 'forward'), (14, 0, 'nontree'), (14, 19, 'forward'), (19, 14, 'nontree'), (19, 1, 'forward'), (1, 10, 'forward'), (10, 12, 'forward'), (12, 9, 'forward'), (9, 12, 'nontree'), (12, 9, 'reverse'), (12, 2, 'forward'), (2, 11, 'forward'), (11, 3, 'forward'), (3, 17, 'forward'), (17, 3, 'nontree'), (3, 17, 'reverse'), (3, 11, 'nontree'), (11, 3, 'reverse'), (11, 18, 'forward'), (18, 4, 'forward'), (4, 8, 'forward'), (8, 6, 'forward'), (6, 8, 'nontree'), (8, 6, 'reverse'), (8, 4, 'nontree'), (4, 8, 'reverse'), (4, 18, 'nontree'), (18, 4, 'reverse'), (18, 16, 'forward'), (16, 13, 'forward'), (13, 5, 'forward'), (5, 7, 'forward'), (7, 5, 'nontree'), (5, 7, 'reverse'), (5, 15, 'forward'), (15, 5, 'nontree'), (5, 15, 'reverse'), (5, 13, 'nontree'), (13, 5, 'reverse'), (13, 16, 'nontree'), (16, 13, 'reverse'), (16, 18, 'nontree'), (18, 16, 'reverse'), (18, 11, 'nontree'), (11, 18, 'reverse'), (11, 2, 'nontree'), (2, 11, 'reverse'), (2, 12, 'nontree'), (12, 2, 'reverse'), (12, 10, 'nontree'), (10, 12, 'reverse'), (10, 1, 'nontree'), (1, 10, 'reverse'), (1, 19, 'nontree'), (19, 1, 'reverse'), (14, 19, 'reverse'), (0, 14, 'reverse'), (0, 0, 'reverse')] 3.2 Breadth-first search Basic algorithms for breadth-first searching the nodes of a graph. bfs_edges(G, source[, reverse, depth_limit]) Iterate over edges in a breadth-first-search starting at source. bfs_tree(G, source[, reverse, depth_limit]) Returns an oriented tree constructed from of a breadth-first-search starting at source. bfs_predecessors(G, source[, depth_limit]) Returns an iterator of predecessors in breadth-first-search from source. bfs_successors(G, source[, depth_limit]) Returns an iterator of successors in breadth-first-search from so print ( list (nx . bfs_edges(G, 0 ))) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12), (12, 9), (12, 2), (2, 11), (11, 3), (11, 18), (3, 17), (18, 4), (18, 16), (4, 8), (16, 13), (8, 6), (13, 5), (5, 7), (5, 15)] nx . draw(nx . bfs_tree(G, 0 ), node_color = 'lightblue' , node_size = 200 , with_labels = True ) print ( list (nx . bfs_predecessors(G, 0 ))) [(14, 0), (19, 14), (1, 19), (10, 1), (12, 10), (9, 12), (2, 12), (11, 2), (3, 11), (18, 11), (17, 3), (4, 18), (16, 18), (8, 4), (13, 16), (6, 8), (5, 13), (7, 5), (15, 5)] print ( list (nx . bfs_successors(G, 0 ))) [(0, [14]), (14, [19]), (19, [1]), (1, [10]), (10, [12]), (12, [9, 2]), (2, [11]), (11, [3, 18]), (3, [17]), (18, [4, 16]), (4, [8]), (16, [13]), (8, [6]), (13, [5]), (5, [7, 15])]","title":"Algorithms - I"},{"location":"algorithms/algI/Algorithms-I/#algorithms-i","text":"","title":"Algorithms - I"},{"location":"algorithms/algI/Algorithms-I/#distance-centrality-community-and-traversal","text":"import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( \"ignore\" ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G)","title":"Distance, Centrality, Community and Traversal"},{"location":"algorithms/algI/Algorithms-I/#1-distance","text":"Graph diameter, radius, eccentricity and other properties. center(G[, e, usebounds]) Returns the center of the graph G. diameter(G[, e, usebounds]) Returns the diameter of the graph G. eccentricity(G[, v, sp]) Returns the eccentricity of nodes in G. extrema_bounding(G[, compute]) Compute requested extreme distance metric of undirected graph G periphery(G[, e, usebounds]) Returns the periphery of the graph G. radius(G[, e, usebounds]) Returns the radius of the graph G. nx . radius(G), nx . diameter(G) (3, 5) nx . center(G) [0, 1, 2, 3, 8, 13, 19, 31] nx . eccentricity(G) {0: 3, 1: 3, 2: 3, 3: 3, 4: 4, 5: 4, 6: 4, 7: 4, 8: 3, 9: 4, 10: 4, 11: 4, 12: 4, 13: 3, 14: 5, 15: 5, 16: 5, 17: 4, 18: 5, 19: 3, 20: 5, 21: 4, 22: 5, 23: 5, 24: 4, 25: 4, 26: 5, 27: 4, 28: 4, 29: 5, 30: 4, 31: 3, 32: 4, 33: 4} nx . extrema_bounding(G) 5 nx . periphery(G) [14, 15, 16, 18, 20, 22, 23, 26, 29]","title":"1. Distance"},{"location":"algorithms/algI/Algorithms-I/#centrality","text":"","title":"Centrality"},{"location":"algorithms/algI/Algorithms-I/#degree","text":"degree_centrality(G) Compute the degree centrality for nodes. in_degree_centrality(G) Compute the in-degree centrality for nodes. out_degree_centrality(G) Compute the out-degree centrality for nodes. G = nx . balanced_tree(r = 3 , h = 2 ) nx . draw(G, node_size = 500 ,node_color = 'lightblue' , with_labels = True ) nx . degree_centrality(G) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . in_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . out_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333}","title":"Degree"},{"location":"algorithms/algI/Algorithms-I/#eigenvector-centality","text":"eigenvector_centrality(G[, max_iter, tol, \u2026]) Compute the eigenvector centrality for the graph G. eigenvector_centrality_numpy(G[, weight, \u2026]) Compute the eigenvector centrality for the graph G. katz_centrality(G[, alpha, beta, max_iter, \u2026]) Compute the Katz centrality for the nodes of the graph G. katz_centrality_numpy(G[, alpha, beta, \u2026]) Compute the Katz centrality for the graph G. nx . eigenvector_centrality(G) {0: 0.49999972705204543, 1: 0.40824851115143324, 2: 0.40824851115143324, 3: 0.40824851115143324, 4: 0.16666657745857444, 5: 0.16666657745857444, 6: 0.16666657745857444, 7: 0.16666657745857444, 8: 0.16666657745857444, 9: 0.16666657745857444, 10: 0.16666657745857444, 11: 0.16666657745857444, 12: 0.16666657745857444} nx . katz_centrality(G) {0: 0.31855094383728, 1: 0.3279200842087277, 2: 0.3279200842087277, 3: 0.3279200842087277, 4: 0.2529669612371458, 5: 0.2529669612371458, 6: 0.2529669612371458, 7: 0.2529669612371458, 8: 0.2529669612371458, 9: 0.2529669612371458, 10: 0.2529669612371458, 11: 0.2529669612371458, 12: 0.2529669612371458}","title":"Eigenvector Centality"},{"location":"algorithms/algI/Algorithms-I/#betweenness-centrality","text":"betweenness_centrality(G[, k, normalized, \u2026]) Compute the shortest-path betweenness centrality for nodes. edge_betweenness_centrality(G[, k, \u2026]) Compute betweenness centrality for edges. betweenness_centrality_subset(G, sources, \u2026) Compute betweenness centrality for a subset of nodes. edge_betweenness_centrality_subset(G, \u2026[, \u2026]) Compute betweenness centrality for edges for a subset of nodes. nx . betweenness_centrality(G) {0: 0.7272727272727273, 1: 0.4545454545454546, 2: 0.4545454545454546, 3: 0.4545454545454546, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0}","title":"Betweenness Centrality"},{"location":"algorithms/algI/Algorithms-I/#2-community","text":"Functions for computing and measuring community structure. The functions in this class are not imported into the top-level networkx namespace. You can access these functions by importing the networkx.algorithms.community module, then accessing the functions as attributes of community. For example: from networkx.algorithms import community G = nx . barbell_graph( 5 , 1 ) nx . draw(G) communities_generator = community . girvan_newman(G) top_level_communities = next (communities_generator) next_level_communities = next (communities_generator) sorted ( map ( sorted , next_level_communities)) [[0, 1, 2, 3, 4], [5], [6, 7, 8, 9, 10]]","title":"2. Community"},{"location":"algorithms/algI/Algorithms-I/#k-clique-communty","text":"G = nx . relaxed_caveman_graph( 5 , 10 , 0.1 ) nx . draw(G, node_size = 40 ,node_color = 'blue' ) from networkx.algorithms.community import k_clique_communities c = list (k_clique_communities(G,k = 4 )) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import greedy_modularity_communities c = list (greedy_modularity_communities(G)) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import asyn_lpa_communities c = list (asyn_lpa_communities(G)) print (c) [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}] from networkx.algorithms.community import label_propagation_communities c = list (label_propagation_communities(G)) print (c) [{10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}] from networkx.algorithms.community import asyn_fluidc c = list (asyn_fluidc(G,k = 5 )) print (c) [{16, 17, 19, 11}, {10, 12, 13, 14, 15, 18}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 40, 9, 8, 43, 44, 45, 46, 47, 48, 49, 41, 42}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}]","title":"K-Clique Communty"},{"location":"algorithms/algI/Algorithms-I/#3-traversal","text":"","title":"3 Traversal"},{"location":"algorithms/algI/Algorithms-I/#31-depth-first-search","text":"Basic algorithms for depth-first searching the nodes of a graph. dfs_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS). dfs_tree(G[, source, depth_limit]) Returns oriented tree constructed from a depth-first-search from source. dfs_predecessors(G[, source, depth_limit]) Returns dictionary of predecessors in depth-first-search from source. dfs_successors(G[, source, depth_limit]) Returns dictionary of successors in depth-first-search from source. dfs_preorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search pre-ordering starting at source. dfs_postorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search post-ordering starting at source. dfs_labeled_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS) labeled by type. G = nx . random_tree( 20 ) nx . draw(G, node_size = 200 ,node_color = 'lightblue' ,with_labels = True ) L = list (nx . dfs_edges(G, source = 0 , depth_limit = 5 )) print (L) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12)] TG = nx . dfs_tree(G) nx . draw(G, node_size = 200 ,node_color = 'lightblue' ,with_labels = True ) nx . dfs_predecessors(G, source = 0 ) {1: 19, 2: 12, 3: 11, 4: 18, 5: 13, 6: 8, 7: 5, 8: 4, 9: 12, 10: 1, 11: 2, 12: 10, 13: 16, 14: 0, 15: 5, 16: 18, 17: 3, 18: 11, 19: 14} nx . dfs_successors(G, source = 0 ) {0: [14], 1: [10], 2: [11], 3: [17], 4: [8], 5: [7, 15], 8: [6], 10: [12], 11: [3, 18], 12: [9, 2], 13: [5], 14: [19], 16: [13], 18: [4, 16], 19: [1]} list (nx . dfs_preorder_nodes(G)) [0, 14, 19, 1, 10, 12, 9, 2, 11, 3, 17, 18, 4, 8, 6, 16, 13, 5, 7, 15] list (nx . dfs_postorder_nodes(G)) [9, 17, 3, 6, 8, 4, 7, 15, 5, 13, 16, 18, 11, 2, 12, 10, 1, 19, 14, 0] list (nx . dfs_labeled_edges(G)) [(0, 0, 'forward'), (0, 14, 'forward'), (14, 0, 'nontree'), (14, 19, 'forward'), (19, 14, 'nontree'), (19, 1, 'forward'), (1, 10, 'forward'), (10, 12, 'forward'), (12, 9, 'forward'), (9, 12, 'nontree'), (12, 9, 'reverse'), (12, 2, 'forward'), (2, 11, 'forward'), (11, 3, 'forward'), (3, 17, 'forward'), (17, 3, 'nontree'), (3, 17, 'reverse'), (3, 11, 'nontree'), (11, 3, 'reverse'), (11, 18, 'forward'), (18, 4, 'forward'), (4, 8, 'forward'), (8, 6, 'forward'), (6, 8, 'nontree'), (8, 6, 'reverse'), (8, 4, 'nontree'), (4, 8, 'reverse'), (4, 18, 'nontree'), (18, 4, 'reverse'), (18, 16, 'forward'), (16, 13, 'forward'), (13, 5, 'forward'), (5, 7, 'forward'), (7, 5, 'nontree'), (5, 7, 'reverse'), (5, 15, 'forward'), (15, 5, 'nontree'), (5, 15, 'reverse'), (5, 13, 'nontree'), (13, 5, 'reverse'), (13, 16, 'nontree'), (16, 13, 'reverse'), (16, 18, 'nontree'), (18, 16, 'reverse'), (18, 11, 'nontree'), (11, 18, 'reverse'), (11, 2, 'nontree'), (2, 11, 'reverse'), (2, 12, 'nontree'), (12, 2, 'reverse'), (12, 10, 'nontree'), (10, 12, 'reverse'), (10, 1, 'nontree'), (1, 10, 'reverse'), (1, 19, 'nontree'), (19, 1, 'reverse'), (14, 19, 'reverse'), (0, 14, 'reverse'), (0, 0, 'reverse')]","title":"3.1 Depth First Search"},{"location":"algorithms/algI/Algorithms-I/#32-breadth-first-search","text":"Basic algorithms for breadth-first searching the nodes of a graph. bfs_edges(G, source[, reverse, depth_limit]) Iterate over edges in a breadth-first-search starting at source. bfs_tree(G, source[, reverse, depth_limit]) Returns an oriented tree constructed from of a breadth-first-search starting at source. bfs_predecessors(G, source[, depth_limit]) Returns an iterator of predecessors in breadth-first-search from source. bfs_successors(G, source[, depth_limit]) Returns an iterator of successors in breadth-first-search from so print ( list (nx . bfs_edges(G, 0 ))) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12), (12, 9), (12, 2), (2, 11), (11, 3), (11, 18), (3, 17), (18, 4), (18, 16), (4, 8), (16, 13), (8, 6), (13, 5), (5, 7), (5, 15)] nx . draw(nx . bfs_tree(G, 0 ), node_color = 'lightblue' , node_size = 200 , with_labels = True ) print ( list (nx . bfs_predecessors(G, 0 ))) [(14, 0), (19, 14), (1, 19), (10, 1), (12, 10), (9, 12), (2, 12), (11, 2), (3, 11), (18, 11), (17, 3), (4, 18), (16, 18), (8, 4), (13, 16), (6, 8), (5, 13), (7, 5), (15, 5)] print ( list (nx . bfs_successors(G, 0 ))) [(0, [14]), (14, [19]), (19, [1]), (1, [10]), (10, [12]), (12, [9, 2]), (2, [11]), (11, [3, 18]), (3, [17]), (18, [4, 16]), (4, [8]), (16, [13]), (8, [6]), (13, [5]), (5, [7, 15])]","title":"3.2 Breadth-first search"},{"location":"algorithms/algII/Algorithms-II/","text":"Algorithm - II Clustering, Link Analysis, Node Classification, Link Prediction import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( \"ignore\" ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) Clustering Algorithms to characterize the number of triangles in a graph. triangles(G[, nodes]) Compute the number of triangles. transitivity(G) Compute graph transitivity, the fraction of all possible triangles present in G. clustering(G[, nodes, weight]) Compute the clustering coefficient for nodes. average_clustering(G[, nodes, weight, \u2026]) Compute the average clustering coefficient for the graph G. square_clustering(G[, nodes]) Compute the squares clustering coefficient for nodes. generalized_degree(G[, nodes]) Compute the generalized degree for nodes. nx . triangles(G) {0: 18, 1: 12, 2: 11, 3: 10, 4: 2, 5: 3, 6: 3, 7: 6, 8: 5, 9: 0, 10: 2, 11: 0, 12: 1, 13: 6, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 4, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 4, 30: 3, 31: 3, 32: 13, 33: 15} nx . transitivity(G) 0.2556818181818182 nx . clustering(G) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} Link Analysis PageRank PageRank analysis of graph structure. pagerank(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_numpy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_scipy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. google_matrix(G[, alpha, personalization, \u2026]) Returns the Google matrix of the graph. nx . pagerank(G) {0: 0.09700181758983709, 1: 0.05287839103742701, 2: 0.057078423047636745, 3: 0.03586064322306479, 4: 0.021979406974834498, 5: 0.02911334166344221, 6: 0.02911334166344221, 7: 0.024490758039509182, 8: 0.029765339186167028, 9: 0.014308950284462801, 10: 0.021979406974834498, 11: 0.009564916863537148, 12: 0.014645186487916191, 13: 0.029536314977202986, 14: 0.014535161524273825, 15: 0.014535161524273825, 16: 0.016785378110253487, 17: 0.014558859774243493, 18: 0.014535161524273825, 19: 0.019604416711937293, 20: 0.014535161524273825, 21: 0.014558859774243493, 22: 0.014535161524273825, 23: 0.03152091531163228, 24: 0.021075455001162945, 25: 0.021005628174745786, 26: 0.015043395360629753, 27: 0.025638803528350497, 28: 0.01957296050943854, 29: 0.02628726283711208, 30: 0.02458933653429248, 31: 0.03715663592267942, 32: 0.07169213006588289, 33: 0.1009179167487121} nx . google_matrix(G) matrix([[0.00441176, 0.05753676, 0.05753676, ..., 0.05753676, 0.00441176, 0.00441176], [0.09885621, 0.00441176, 0.09885621, ..., 0.00441176, 0.00441176, 0.00441176], [0.08941176, 0.08941176, 0.00441176, ..., 0.00441176, 0.08941176, 0.00441176], ..., [0.14607843, 0.00441176, 0.00441176, ..., 0.00441176, 0.14607843, 0.14607843], [0.00441176, 0.00441176, 0.0752451 , ..., 0.0752451 , 0.00441176, 0.0752451 ], [0.00441176, 0.00441176, 0.00441176, ..., 0.05441176, 0.05441176, 0.00441176]]) Hits Hubs and authorities analysis of graph structure. hits(G[, max_iter, tol, nstart, normalized]) Returns HITS hubs and authorities values for nodes. hits_numpy(G[, normalized]) Returns HITS hubs and authorities values for nodes. hits_scipy(G[, max_iter, tol, normalized]) Returns HITS hubs and authorities values for nodes. hub_matrix(G[, nodelist]) Returns the HITS hub matrix. authority_matrix(G[, nodelist]) Returns the HITS authority matrix. nx . hits(G) ({0: 0.07141272875773573, 1: 0.053427231205172614, 2: 0.06371906453963268, 3: 0.04242273710428976, 4: 0.01526095969815266, 5: 0.015966913494418547, 6: 0.015966913494418547, 7: 0.034343167206797434, 8: 0.0456819251308063, 9: 0.020625667757182626, 10: 0.01526095969815266, 11: 0.01061789150852051, 12: 0.01692545078543599, 13: 0.04549486406600547, 14: 0.020370345842716076, 15: 0.020370345842716076, 16: 0.004748031841562519, 17: 0.018561637031907358, 18: 0.020370345842716076, 19: 0.02971333389111539, 20: 0.020370345842716076, 21: 0.018561637031907358, 22: 0.020370345842716076, 23: 0.030156497528902444, 24: 0.011460952230139869, 25: 0.01189366439609368, 26: 0.015182734341447207, 27: 0.02681349412708363, 28: 0.0263315057833753, 29: 0.027111539646424865, 30: 0.03510623798827733, 31: 0.03837574188047834, 32: 0.06200184647463986, 33: 0.07500294214634279}, {0: 0.07141272880870855, 1: 0.05342723122870397, 2: 0.06371906455587135, 3: 0.04242273710611524, 4: 0.015260959692251741, 5: 0.01596691348769785, 6: 0.01596691348769785, 7: 0.03434316719678568, 8: 0.045681925113766106, 9: 0.020625667747004237, 10: 0.015260959692251741, 11: 0.010617891499780771, 12: 0.016925450777611116, 13: 0.045494864044925934, 14: 0.02037034582705704, 15: 0.02037034582705704, 16: 0.004748031844529441, 17: 0.01856163702009135, 18: 0.02037034582705704, 19: 0.029713333868231606, 20: 0.02037034582705704, 21: 0.01856163702009135, 22: 0.02037034582705704, 23: 0.030156497522138854, 24: 0.011460952243147787, 25: 0.011893664411194165, 26: 0.015182734336172116, 27: 0.026813494122100573, 28: 0.026331505783102067, 29: 0.02711153964098065, 30: 0.03510623797808329, 31: 0.03837574185646307, 32: 0.06200184653550559, 33: 0.0750029422437107}) nx . hub_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]]) nx . authority_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]]) Node Classification This module provides the functions for node classification problem. The functions in this module are not imported into the top level networkx namespace. You can access these functions by importing the networkx.algorithms.node_classification modules, then accessing the functions as attributes of node_classification. For example: import networkx as nx from networkx.algorithms import node_classification G = nx . balanced_tree( 3 , 3 ) nx . draw(G, node_size = 500 , node_color = \"lightgreen\" , with_labels = True ) G . node[ 1 ][ 'label' ] = 'A' G . node[ 2 ][ 'label' ] = 'B' G . node[ 3 ][ 'label' ] = 'C' L = node_classification . harmonic_function(G) print (L) ['A', 'A', 'B', 'C', 'A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] LL = {} for n,l in zip (G . nodes(),L): LL . update({n:l}) nx . draw(G, node_size = 500 , labels = LL, node_color = \"lightgreen\" , with_labels = True ) Link Prediction Link prediction algorithms. resource_allocation_index(G[, ebunch]) Compute the resource allocation index of all node pairs in ebunch. jaccard_coefficient(G[, ebunch]) Compute the Jaccard coefficient of all node pairs in ebunch. adamic_adar_index(G[, ebunch]) Compute the Adamic-Adar index of all node pairs in ebunch. preferential_attachment(G[, ebunch]) Compute the preferential attachment score of all node pairs in ebunch. cn_soundarajan_hopcroft(G[, ebunch, community]) Count the number of common neighbors of all node pairs in ebunch ra_index_soundarajan_hopcroft(G[, ebunch, \u2026]) Compute the resource allocation index of all node pairs in ebunch using community information. within_inter_cluster(G[, ebunch, delta, \u2026]) Compute the ratio of within- and inter-cluster common neighb G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) preds = nx . resource_allocation_index(G, [( 0 , 10 ),( 9 , 18 ), ( 11 , 12 ),( 30 , 27 ),( 16 , 26 )]) for u, v, p in preds: print ( '( %d , %d ) -> %.8f ' % (u, v, p)) (0, 10) -&gt; 0.58333333 (9, 18) -&gt; 0.05882353 (11, 12) -&gt; 0.06250000 (30, 27) -&gt; 0.05882353 (16, 26) -&gt; 0.00000000","title":"Algorithms - II"},{"location":"algorithms/algII/Algorithms-II/#algorithm-ii","text":"","title":"Algorithm - II"},{"location":"algorithms/algII/Algorithms-II/#clustering-link-analysis-node-classification-link-prediction","text":"import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( \"ignore\" ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True )","title":"Clustering, Link Analysis, Node Classification, Link Prediction"},{"location":"algorithms/algII/Algorithms-II/#clustering","text":"Algorithms to characterize the number of triangles in a graph. triangles(G[, nodes]) Compute the number of triangles. transitivity(G) Compute graph transitivity, the fraction of all possible triangles present in G. clustering(G[, nodes, weight]) Compute the clustering coefficient for nodes. average_clustering(G[, nodes, weight, \u2026]) Compute the average clustering coefficient for the graph G. square_clustering(G[, nodes]) Compute the squares clustering coefficient for nodes. generalized_degree(G[, nodes]) Compute the generalized degree for nodes. nx . triangles(G) {0: 18, 1: 12, 2: 11, 3: 10, 4: 2, 5: 3, 6: 3, 7: 6, 8: 5, 9: 0, 10: 2, 11: 0, 12: 1, 13: 6, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 4, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 4, 30: 3, 31: 3, 32: 13, 33: 15} nx . transitivity(G) 0.2556818181818182 nx . clustering(G) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882}","title":"Clustering"},{"location":"algorithms/algII/Algorithms-II/#link-analysis","text":"","title":"Link Analysis"},{"location":"algorithms/algII/Algorithms-II/#pagerank","text":"PageRank analysis of graph structure. pagerank(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_numpy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_scipy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. google_matrix(G[, alpha, personalization, \u2026]) Returns the Google matrix of the graph. nx . pagerank(G) {0: 0.09700181758983709, 1: 0.05287839103742701, 2: 0.057078423047636745, 3: 0.03586064322306479, 4: 0.021979406974834498, 5: 0.02911334166344221, 6: 0.02911334166344221, 7: 0.024490758039509182, 8: 0.029765339186167028, 9: 0.014308950284462801, 10: 0.021979406974834498, 11: 0.009564916863537148, 12: 0.014645186487916191, 13: 0.029536314977202986, 14: 0.014535161524273825, 15: 0.014535161524273825, 16: 0.016785378110253487, 17: 0.014558859774243493, 18: 0.014535161524273825, 19: 0.019604416711937293, 20: 0.014535161524273825, 21: 0.014558859774243493, 22: 0.014535161524273825, 23: 0.03152091531163228, 24: 0.021075455001162945, 25: 0.021005628174745786, 26: 0.015043395360629753, 27: 0.025638803528350497, 28: 0.01957296050943854, 29: 0.02628726283711208, 30: 0.02458933653429248, 31: 0.03715663592267942, 32: 0.07169213006588289, 33: 0.1009179167487121} nx . google_matrix(G) matrix([[0.00441176, 0.05753676, 0.05753676, ..., 0.05753676, 0.00441176, 0.00441176], [0.09885621, 0.00441176, 0.09885621, ..., 0.00441176, 0.00441176, 0.00441176], [0.08941176, 0.08941176, 0.00441176, ..., 0.00441176, 0.08941176, 0.00441176], ..., [0.14607843, 0.00441176, 0.00441176, ..., 0.00441176, 0.14607843, 0.14607843], [0.00441176, 0.00441176, 0.0752451 , ..., 0.0752451 , 0.00441176, 0.0752451 ], [0.00441176, 0.00441176, 0.00441176, ..., 0.05441176, 0.05441176, 0.00441176]])","title":"PageRank"},{"location":"algorithms/algII/Algorithms-II/#hits","text":"Hubs and authorities analysis of graph structure. hits(G[, max_iter, tol, nstart, normalized]) Returns HITS hubs and authorities values for nodes. hits_numpy(G[, normalized]) Returns HITS hubs and authorities values for nodes. hits_scipy(G[, max_iter, tol, normalized]) Returns HITS hubs and authorities values for nodes. hub_matrix(G[, nodelist]) Returns the HITS hub matrix. authority_matrix(G[, nodelist]) Returns the HITS authority matrix. nx . hits(G) ({0: 0.07141272875773573, 1: 0.053427231205172614, 2: 0.06371906453963268, 3: 0.04242273710428976, 4: 0.01526095969815266, 5: 0.015966913494418547, 6: 0.015966913494418547, 7: 0.034343167206797434, 8: 0.0456819251308063, 9: 0.020625667757182626, 10: 0.01526095969815266, 11: 0.01061789150852051, 12: 0.01692545078543599, 13: 0.04549486406600547, 14: 0.020370345842716076, 15: 0.020370345842716076, 16: 0.004748031841562519, 17: 0.018561637031907358, 18: 0.020370345842716076, 19: 0.02971333389111539, 20: 0.020370345842716076, 21: 0.018561637031907358, 22: 0.020370345842716076, 23: 0.030156497528902444, 24: 0.011460952230139869, 25: 0.01189366439609368, 26: 0.015182734341447207, 27: 0.02681349412708363, 28: 0.0263315057833753, 29: 0.027111539646424865, 30: 0.03510623798827733, 31: 0.03837574188047834, 32: 0.06200184647463986, 33: 0.07500294214634279}, {0: 0.07141272880870855, 1: 0.05342723122870397, 2: 0.06371906455587135, 3: 0.04242273710611524, 4: 0.015260959692251741, 5: 0.01596691348769785, 6: 0.01596691348769785, 7: 0.03434316719678568, 8: 0.045681925113766106, 9: 0.020625667747004237, 10: 0.015260959692251741, 11: 0.010617891499780771, 12: 0.016925450777611116, 13: 0.045494864044925934, 14: 0.02037034582705704, 15: 0.02037034582705704, 16: 0.004748031844529441, 17: 0.01856163702009135, 18: 0.02037034582705704, 19: 0.029713333868231606, 20: 0.02037034582705704, 21: 0.01856163702009135, 22: 0.02037034582705704, 23: 0.030156497522138854, 24: 0.011460952243147787, 25: 0.011893664411194165, 26: 0.015182734336172116, 27: 0.026813494122100573, 28: 0.026331505783102067, 29: 0.02711153964098065, 30: 0.03510623797808329, 31: 0.03837574185646307, 32: 0.06200184653550559, 33: 0.0750029422437107}) nx . hub_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]]) nx . authority_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]])","title":"Hits"},{"location":"algorithms/algII/Algorithms-II/#node-classification","text":"This module provides the functions for node classification problem. The functions in this module are not imported into the top level networkx namespace. You can access these functions by importing the networkx.algorithms.node_classification modules, then accessing the functions as attributes of node_classification. For example: import networkx as nx from networkx.algorithms import node_classification G = nx . balanced_tree( 3 , 3 ) nx . draw(G, node_size = 500 , node_color = \"lightgreen\" , with_labels = True ) G . node[ 1 ][ 'label' ] = 'A' G . node[ 2 ][ 'label' ] = 'B' G . node[ 3 ][ 'label' ] = 'C' L = node_classification . harmonic_function(G) print (L) ['A', 'A', 'B', 'C', 'A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] LL = {} for n,l in zip (G . nodes(),L): LL . update({n:l}) nx . draw(G, node_size = 500 , labels = LL, node_color = \"lightgreen\" , with_labels = True )","title":"Node Classification"},{"location":"algorithms/algII/Algorithms-II/#link-prediction","text":"Link prediction algorithms. resource_allocation_index(G[, ebunch]) Compute the resource allocation index of all node pairs in ebunch. jaccard_coefficient(G[, ebunch]) Compute the Jaccard coefficient of all node pairs in ebunch. adamic_adar_index(G[, ebunch]) Compute the Adamic-Adar index of all node pairs in ebunch. preferential_attachment(G[, ebunch]) Compute the preferential attachment score of all node pairs in ebunch. cn_soundarajan_hopcroft(G[, ebunch, community]) Count the number of common neighbors of all node pairs in ebunch ra_index_soundarajan_hopcroft(G[, ebunch, \u2026]) Compute the resource allocation index of all node pairs in ebunch using community information. within_inter_cluster(G[, ebunch, delta, \u2026]) Compute the ratio of within- and inter-cluster common neighb G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) preds = nx . resource_allocation_index(G, [( 0 , 10 ),( 9 , 18 ), ( 11 , 12 ),( 30 , 27 ),( 16 , 26 )]) for u, v, p in preds: print ( '( %d , %d ) -> %.8f ' % (u, v, p)) (0, 10) -&gt; 0.58333333 (9, 18) -&gt; 0.05882353 (11, 12) -&gt; 0.06250000 (30, 27) -&gt; 0.05882353 (16, 26) -&gt; 0.00000000","title":"Link Prediction"},{"location":"algorithms/algIII/Algorithms-III/","text":"Algorithm - III Operators, Shortest Paths, Trees, Planarity, Flows, Directed Acyclic Graphs, Approximations and Heuristics, Assortativity import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( \"ignore\" ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) Operators Unary operations on graphs complement(G) Returns the graph complement of G. reverse(G[, copy]) Returns the reverse directed graph of G. Operations on graphs including union, intersection, difference. compose(G, H) Returns a new graph of G composed with H. union(G, H[, rename, name]) Return the union of graphs G and H. disjoint_union(G, H) Return the disjoint union of graphs G and H. intersection(G, H) Returns a new graph that contains only the edges that exist in both G and H. difference(G, H) Returns a new graph that contains the edges that exist in G but not in H. symmetric_difference(G, H) Returns new graph with edges that exist in either G or H but not both. Operations on many graphs. compose_all(graphs) Returns the composition of all graphs. union_all(graphs[, rename]) Returns the union of all graphs. disjoint_union_all(graphs) Returns the disjoint union of all graphs. intersection_all(graphs) Returns a new graph that contains only the edges that exist in all graphs. Graph products. cartesian_product(G, H) Returns the Cartesian product of G and H. lexicographic_product(G, H) Returns the lexicographic product of G and H. rooted_product(G, H, root) Return the rooted product of graphs G and H rooted at root in H. strong_product(G, H) Returns the strong product of G and H. tensor_product(G, H) Returns the tensor product of G and H. power(G, k) Returns the specified power of a graph. CG = nx . complement(G) nx . draw(CG, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) RG = nx . reverse(nx . to_directed(G)) nx . draw(RG, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) G = nx . balanced_tree( 2 , 3 ) H = nx . balanced_tree( 3 , 2 ) plt . figure(figsize = [ 20 , 8 ]) plt . subplot( 1 , 3 , 1 ) nx . draw(G, node_size = 500 ,node_color = 'lightblue' , with_labels = True ) plt . subplot( 1 , 3 , 2 ) nx . draw(G, node_size = 500 ,node_color = 'lightgreen' , with_labels = True ) plt . subplot( 1 , 3 , 3 ) nx . draw(nx . compose(H,G), node_size = 500 , node_color = 'magenta' , with_labels = True ) G = nx . balanced_tree( 3 , 2 ) nx . draw(nx . power(G,k = 3 ), node_size = 500 , node_color = 'magenta' , with_labels = True ) Shortest Paths Compute the shortest paths and path lengths between nodes in the graph. These algorithms work with undirected and directed graphs. shortest_path(G[, source, target, weight, \u2026]) Compute shortest paths in the graph. all_shortest_paths(G, source, target[, \u2026]) Compute all shortest paths in the graph. shortest_path_length(G[, source, target, \u2026]) Compute shortest path lengths in the graph. average_shortest_path_length(G[, weight, method]) Returns the average shortest path length. has_path(G, source, target) Returns True if G has a path from source to target. Advanced Interface Shortest path algorithms for unweighted graphs. single_source_shortest_path(G, source[, cutoff]) Compute shortest path between source and all other nodes reachable from source. single_source_shortest_path_length(G, source) Compute the shortest path lengths from source to all reachable nodes. single_target_shortest_path(G, target[, cutoff]) Compute shortest path to target from all nodes that reach target. single_target_shortest_path_length(G, target) Compute the shortest path lengths to target from all reachable nodes. bidirectional_shortest_path(G, source, target) Returns a list of nodes in a shortest path between source and target. all_pairs_shortest_path(G[, cutoff]) Compute shortest paths between all nodes. all_pairs_shortest_path_length(G[, cutoff]) Computes the shortest path lengths between all nodes in G. predecessor(G, source[, target, cutoff, \u2026]) Returns dict of predecessors for the path from source to all nodes in G Shortest path algorithms for weighed graphs. dijkstra_predecessor_and_distance(G, source) Compute weighted shortest path length and predecessors. dijkstra_path(G, source, target[, weight]) Returns the shortest weighted path from source to target in G. dijkstra_path_length(G, source, target[, weight]) Returns the shortest weighted path length in G from source to target. single_source_dijkstra(G, source[, target, \u2026]) Find shortest weighted paths and lengths from a source node. single_source_dijkstra_path(G, source[, \u2026]) Find shortest weighted paths in G from a source node. single_source_dijkstra_path_length(G, source) Find shortest weighted path lengths in G from a source node. multi_source_dijkstra(G, sources[, target, \u2026]) Find shortest weighted paths and lengths from a given set of source nodes. multi_source_dijkstra_path(G, sources[, \u2026]) Find shortest weighted paths in G from a given set of source nodes. multi_source_dijkstra_path_length(G, sources) Find shortest weighted path lengths in G from a given set of source nodes. all_pairs_dijkstra(G[, cutoff, weight]) Find shortest weighted paths and lengths between all nodes. all_pairs_dijkstra_path(G[, cutoff, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_dijkstra_path_length(G[, cutoff, \u2026]) Compute shortest path lengths between all nodes in a weighted graph. bidirectional_dijkstra(G, source, target[, \u2026]) Dijkstra\u2019s algorithm for shortest paths using bidirectional search. bellman_ford_path(G, source, target[, weight]) Returns the shortest path from source to target in a weighted graph G. bellman_ford_path_length(G, source, target) Returns the shortest path length from source to target in a weighted graph. single_source_bellman_ford(G, source[, \u2026]) Compute shortest paths and lengths in a weighted graph G. single_source_bellman_ford_path(G, source[, \u2026]) Compute shortest path between source and all other reachable nodes for a weighted graph. single_source_bellman_ford_path_length(G, source) Compute the shortest path length between source and all other reachable nodes for a weighted graph. all_pairs_bellman_ford_path(G[, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_bellman_ford_path_length(G[, weight]) Compute shortest path lengths between all nodes in a weighted graph. bellman_ford_predecessor_and_distance(G, source) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. negative_edge_cycle(G[, weight]) Returns True if there exists a negative edge cycle anywhere in G. goldberg_radzik(G, source[, weight]) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. johnson(G[, weight]) Uses Johnson\u2019s Algorithm to compute shortest paths. Dense Graphs Floyd-Warshall algorithm for shortest paths. floyd_warshall(G[, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_predecessor_and_distance(G[, \u2026]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_numpy(G[, nodelist, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. reconstruct_path(source, target, predecessors) Reconstruct a path from source to target using the predecessors dict as returned by floyd_warshall_predecessor_and_distance A* Algorithm Shortest paths and path lengths using the A* (\u201cA star\u201d) algorithm. astar_path(G, source, target[, heuristic, \u2026]) Returns a list of nodes in a shortest path between source and target using the A* (\u201cA-star\u201d) algorithm. astar_path_length(G, source, target[, \u2026]) Returns the length of the shortest path between source and target using t G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) nx . shortest_path(G, 0 , 26 ) [0, 8, 33, 26] list (nx . all_shortest_paths(G, 0 , 26 )) [[0, 8, 33, 26], [0, 13, 33, 26], [0, 19, 33, 26], [0, 31, 33, 26]]","title":"Algorithms - II"},{"location":"algorithms/algIII/Algorithms-III/#algorithm-iii","text":"","title":"Algorithm - III"},{"location":"algorithms/algIII/Algorithms-III/#operators-shortest-paths-trees-planarity-flows-directed-acyclic-graphs-approximations-and-heuristics-assortativity","text":"import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( \"ignore\" ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True )","title":"Operators, Shortest Paths, Trees, Planarity, Flows, Directed Acyclic Graphs, Approximations and Heuristics,  Assortativity"},{"location":"algorithms/algIII/Algorithms-III/#operators","text":"Unary operations on graphs complement(G) Returns the graph complement of G. reverse(G[, copy]) Returns the reverse directed graph of G. Operations on graphs including union, intersection, difference. compose(G, H) Returns a new graph of G composed with H. union(G, H[, rename, name]) Return the union of graphs G and H. disjoint_union(G, H) Return the disjoint union of graphs G and H. intersection(G, H) Returns a new graph that contains only the edges that exist in both G and H. difference(G, H) Returns a new graph that contains the edges that exist in G but not in H. symmetric_difference(G, H) Returns new graph with edges that exist in either G or H but not both. Operations on many graphs. compose_all(graphs) Returns the composition of all graphs. union_all(graphs[, rename]) Returns the union of all graphs. disjoint_union_all(graphs) Returns the disjoint union of all graphs. intersection_all(graphs) Returns a new graph that contains only the edges that exist in all graphs. Graph products. cartesian_product(G, H) Returns the Cartesian product of G and H. lexicographic_product(G, H) Returns the lexicographic product of G and H. rooted_product(G, H, root) Return the rooted product of graphs G and H rooted at root in H. strong_product(G, H) Returns the strong product of G and H. tensor_product(G, H) Returns the tensor product of G and H. power(G, k) Returns the specified power of a graph. CG = nx . complement(G) nx . draw(CG, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) RG = nx . reverse(nx . to_directed(G)) nx . draw(RG, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) G = nx . balanced_tree( 2 , 3 ) H = nx . balanced_tree( 3 , 2 ) plt . figure(figsize = [ 20 , 8 ]) plt . subplot( 1 , 3 , 1 ) nx . draw(G, node_size = 500 ,node_color = 'lightblue' , with_labels = True ) plt . subplot( 1 , 3 , 2 ) nx . draw(G, node_size = 500 ,node_color = 'lightgreen' , with_labels = True ) plt . subplot( 1 , 3 , 3 ) nx . draw(nx . compose(H,G), node_size = 500 , node_color = 'magenta' , with_labels = True ) G = nx . balanced_tree( 3 , 2 ) nx . draw(nx . power(G,k = 3 ), node_size = 500 , node_color = 'magenta' , with_labels = True )","title":"Operators"},{"location":"algorithms/algIII/Algorithms-III/#shortest-paths","text":"Compute the shortest paths and path lengths between nodes in the graph. These algorithms work with undirected and directed graphs. shortest_path(G[, source, target, weight, \u2026]) Compute shortest paths in the graph. all_shortest_paths(G, source, target[, \u2026]) Compute all shortest paths in the graph. shortest_path_length(G[, source, target, \u2026]) Compute shortest path lengths in the graph. average_shortest_path_length(G[, weight, method]) Returns the average shortest path length. has_path(G, source, target) Returns True if G has a path from source to target. Advanced Interface","title":"Shortest Paths"},{"location":"algorithms/algIII/Algorithms-III/#shortest-path-algorithms-for-unweighted-graphs","text":"single_source_shortest_path(G, source[, cutoff]) Compute shortest path between source and all other nodes reachable from source. single_source_shortest_path_length(G, source) Compute the shortest path lengths from source to all reachable nodes. single_target_shortest_path(G, target[, cutoff]) Compute shortest path to target from all nodes that reach target. single_target_shortest_path_length(G, target) Compute the shortest path lengths to target from all reachable nodes. bidirectional_shortest_path(G, source, target) Returns a list of nodes in a shortest path between source and target. all_pairs_shortest_path(G[, cutoff]) Compute shortest paths between all nodes. all_pairs_shortest_path_length(G[, cutoff]) Computes the shortest path lengths between all nodes in G. predecessor(G, source[, target, cutoff, \u2026]) Returns dict of predecessors for the path from source to all nodes in G","title":"Shortest path algorithms for unweighted graphs."},{"location":"algorithms/algIII/Algorithms-III/#shortest-path-algorithms-for-weighed-graphs","text":"dijkstra_predecessor_and_distance(G, source) Compute weighted shortest path length and predecessors. dijkstra_path(G, source, target[, weight]) Returns the shortest weighted path from source to target in G. dijkstra_path_length(G, source, target[, weight]) Returns the shortest weighted path length in G from source to target. single_source_dijkstra(G, source[, target, \u2026]) Find shortest weighted paths and lengths from a source node. single_source_dijkstra_path(G, source[, \u2026]) Find shortest weighted paths in G from a source node. single_source_dijkstra_path_length(G, source) Find shortest weighted path lengths in G from a source node. multi_source_dijkstra(G, sources[, target, \u2026]) Find shortest weighted paths and lengths from a given set of source nodes. multi_source_dijkstra_path(G, sources[, \u2026]) Find shortest weighted paths in G from a given set of source nodes. multi_source_dijkstra_path_length(G, sources) Find shortest weighted path lengths in G from a given set of source nodes. all_pairs_dijkstra(G[, cutoff, weight]) Find shortest weighted paths and lengths between all nodes. all_pairs_dijkstra_path(G[, cutoff, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_dijkstra_path_length(G[, cutoff, \u2026]) Compute shortest path lengths between all nodes in a weighted graph. bidirectional_dijkstra(G, source, target[, \u2026]) Dijkstra\u2019s algorithm for shortest paths using bidirectional search. bellman_ford_path(G, source, target[, weight]) Returns the shortest path from source to target in a weighted graph G. bellman_ford_path_length(G, source, target) Returns the shortest path length from source to target in a weighted graph. single_source_bellman_ford(G, source[, \u2026]) Compute shortest paths and lengths in a weighted graph G. single_source_bellman_ford_path(G, source[, \u2026]) Compute shortest path between source and all other reachable nodes for a weighted graph. single_source_bellman_ford_path_length(G, source) Compute the shortest path length between source and all other reachable nodes for a weighted graph. all_pairs_bellman_ford_path(G[, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_bellman_ford_path_length(G[, weight]) Compute shortest path lengths between all nodes in a weighted graph. bellman_ford_predecessor_and_distance(G, source) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. negative_edge_cycle(G[, weight]) Returns True if there exists a negative edge cycle anywhere in G. goldberg_radzik(G, source[, weight]) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. johnson(G[, weight]) Uses Johnson\u2019s Algorithm to compute shortest paths. Dense Graphs","title":"Shortest path algorithms for weighed graphs."},{"location":"algorithms/algIII/Algorithms-III/#floyd-warshall-algorithm-for-shortest-paths","text":"floyd_warshall(G[, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_predecessor_and_distance(G[, \u2026]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_numpy(G[, nodelist, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. reconstruct_path(source, target, predecessors) Reconstruct a path from source to target using the predecessors dict as returned by floyd_warshall_predecessor_and_distance","title":"Floyd-Warshall algorithm for shortest paths."},{"location":"algorithms/algIII/Algorithms-III/#a-algorithm","text":"Shortest paths and path lengths using the A* (\u201cA star\u201d) algorithm. astar_path(G, source, target[, heuristic, \u2026]) Returns a list of nodes in a shortest path between source and target using the A* (\u201cA-star\u201d) algorithm. astar_path_length(G, source, target[, \u2026]) Returns the length of the shortest path between source and target using t G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = \"lightblue\" , with_labels = True ) nx . shortest_path(G, 0 , 26 ) [0, 8, 33, 26] list (nx . all_shortest_paths(G, 0 , 26 )) [[0, 8, 33, 26], [0, 13, 33, 26], [0, 19, 33, 26], [0, 31, 33, 26]]","title":"A* Algorithm"},{"location":"download/download/","text":"ICD 11 Download Pipeline Setting up API To download ICD11 data you need to use the API provided at: https://icd.who.int/icdapi . In order to gain access to the API you need to create an account and use the client key provided. With the client key you are now able to access all of the endpoints specified in the API documentation . The rest of this guide uses the ICD11 module from the ping lab utils package. You can find and clone the module here: https://github.com/salviStudent/testing/tree/master/testing-master . Working Directory and Additonal Dependencies For the simplest use you need to have a json file named config.json in your working directory. The config file needs to have the following: { \"ClientId\":\"your_client_id\", \"ClientSecret\": \"your_client_secret\" } where your_client_id and your_client_secret are your client and secret keys respectively. Along with this config file ICD11.py only needs the request module to function. You can install it by running pip3 install requests if it is not already installed. Getting started Once all of this is in place you are ready to start downloading ICD-11 data. As an example we show the results from the ICD-11 code corresponding to hypertensive heart disease. from ICD11 import icd11_data hypertensive_heart_disease = icd11_data( \"1210166201\" ) print (hypetensive_heart_disease) This outputs: { '@context' : 'http://id.who.int/icd/contexts/contextForFoundationEntity.json' , '@id' : 'http://id.who.int/icd/entity/1210166201' , 'parent' : [ 'http://id.who.int/icd/entity/924915526' , 'http://id.who.int/icd/entity/1395497138' ], 'child' : [ 'http://id.who.int/icd/entity/600660459' , 'http://id.who.int/icd/entity/1208029865' ], 'browserUrl' : 'NA' , 'title' : { '@language' : 'en' , '@value' : 'Hypertensive heart disease' }, 'synonym' : [ { 'label' : { '@language' : 'en' , '@value' : 'HHD - [hypertensive heart disease]' } }, { 'label' : { '@language' : 'en' , '@value' : 'hypertensive cardiac disease' } } ], 'definition' : { '@language' : 'en' , '@value' : 'Uncontrolled and prolonged hypertension can lead to a variety of changes in the myocardial structure, coronary vasculature, and conduction system of the heart. Hypertensive heart disease is a term applied generally to heart diseases, such as left ventricular hypertrophy, coronary artery disease, cardiac arrhythmias, and congestive heart failure, that are caused by direct or indirect effects hypertension.' } }","title":"Download Pipeline"},{"location":"download/download/#icd-11-download-pipeline","text":"","title":"ICD 11 Download Pipeline"},{"location":"download/download/#setting-up-api","text":"To download ICD11 data you need to use the API provided at: https://icd.who.int/icdapi . In order to gain access to the API you need to create an account and use the client key provided. With the client key you are now able to access all of the endpoints specified in the API documentation . The rest of this guide uses the ICD11 module from the ping lab utils package. You can find and clone the module here: https://github.com/salviStudent/testing/tree/master/testing-master .","title":"Setting up API"},{"location":"download/download/#working-directory-and-additonal-dependencies","text":"For the simplest use you need to have a json file named config.json in your working directory. The config file needs to have the following: { \"ClientId\":\"your_client_id\", \"ClientSecret\": \"your_client_secret\" } where your_client_id and your_client_secret are your client and secret keys respectively. Along with this config file ICD11.py only needs the request module to function. You can install it by running pip3 install requests if it is not already installed.","title":"Working Directory and Additonal Dependencies"},{"location":"download/download/#getting-started","text":"Once all of this is in place you are ready to start downloading ICD-11 data. As an example we show the results from the ICD-11 code corresponding to hypertensive heart disease. from ICD11 import icd11_data hypertensive_heart_disease = icd11_data( \"1210166201\" ) print (hypetensive_heart_disease) This outputs: { '@context' : 'http://id.who.int/icd/contexts/contextForFoundationEntity.json' , '@id' : 'http://id.who.int/icd/entity/1210166201' , 'parent' : [ 'http://id.who.int/icd/entity/924915526' , 'http://id.who.int/icd/entity/1395497138' ], 'child' : [ 'http://id.who.int/icd/entity/600660459' , 'http://id.who.int/icd/entity/1208029865' ], 'browserUrl' : 'NA' , 'title' : { '@language' : 'en' , '@value' : 'Hypertensive heart disease' }, 'synonym' : [ { 'label' : { '@language' : 'en' , '@value' : 'HHD - [hypertensive heart disease]' } }, { 'label' : { '@language' : 'en' , '@value' : 'hypertensive cardiac disease' } } ], 'definition' : { '@language' : 'en' , '@value' : 'Uncontrolled and prolonged hypertension can lead to a variety of changes in the myocardial structure, coronary vasculature, and conduction system of the heart. Hypertensive heart disease is a term applied generally to heart diseases, such as left ventricular hypertrophy, coronary artery disease, cardiac arrhythmias, and congestive heart failure, that are caused by direct or indirect effects hypertension.' } }","title":"Getting started"},{"location":"explore/explore/","text":"","title":"Exploratory Data Analysis"},{"location":"flask/flask/","text":"","title":"Flask Implementation"},{"location":"graph/graph/","text":"","title":"Graphical Database"},{"location":"indexing/indexing/","text":"","title":"Indexing"},{"location":"mapping/mapping/","text":"","title":"Mapping Tables"},{"location":"models/clustering/","text":"Coming Soon Under construction","title":"Clustering"},{"location":"models/clustering/#coming-soon","text":"Under construction","title":"Coming Soon"},{"location":"models/link/","text":"Coming Soon Under construction","title":"Link Prediction"},{"location":"models/link/#coming-soon","text":"Under construction","title":"Coming Soon"},{"location":"models/random-walk/","text":"Coming Soon Under construction","title":"Randomwalk"},{"location":"models/random-walk/#coming-soon","text":"Under construction","title":"Coming Soon"},{"location":"node/node/","text":"","title":"Node Implementation"},{"location":"nosql/nosql/","text":"","title":"NoSQL database"},{"location":"parsing/parsing/","text":"","title":"Parsing Pipeline"},{"location":"setup/anaconda/","text":"Installing Python To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python. Note- Linux: For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook Note - Cloud For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Setting up Python"},{"location":"setup/anaconda/#installing-python","text":"To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python.","title":"Installing Python"},{"location":"setup/anaconda/#note-linux","text":"For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook","title":"Note- Linux:"},{"location":"setup/anaconda/#note-cloud","text":"For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Note - Cloud"},{"location":"setup/env/","text":"Python Environment Basics To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = \"/Users/username/anaconda/bin: $PATH \" or update above command line to your .zsh_config file. Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name If you want to update all packages in an environment, which is often useful, use conda update --all And finally, to list installed packages, it's conda list If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup Environments Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate Saving and loading environments A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml . Listing environments If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root . Removing environments If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ). Using environments One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican . Sharing environments When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda. More to learn To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"Setting up Node"},{"location":"setup/env/#python-environment","text":"","title":"Python Environment"},{"location":"setup/env/#basics","text":"To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = \"/Users/username/anaconda/bin: $PATH \" or update above command line to your .zsh_config file. Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name If you want to update all packages in an environment, which is often useful, use conda update --all And finally, to list installed packages, it's conda list If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup","title":"Basics"},{"location":"setup/env/#environments","text":"Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate","title":"Environments"},{"location":"setup/env/#saving-and-loading-environments","text":"A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml .","title":"Saving and loading environments"},{"location":"setup/env/#listing-environments","text":"If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root .","title":"Listing environments"},{"location":"setup/env/#removing-environments","text":"If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ).","title":"Removing environments"},{"location":"setup/env/#using-environments","text":"One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican .","title":"Using environments"},{"location":"setup/env/#sharing-environments","text":"When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda.","title":"Sharing environments"},{"location":"setup/env/#more-to-learn","text":"To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"More to learn"},{"location":"setup/git/","text":"How to git Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m \"First commit\" Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"Setting up Git"},{"location":"setup/git/#how-to-git","text":"Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m \"First commit\" Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"How to git"},{"location":"setup/jupyter/","text":"Installing Jupyter Notebook By far the easiest way to install Jupyter is with Anaconda. Jupyter notebooks automatically come with the distribution. You'll be able to use notebooks from the default environment. To install Jupyter notebooks in a conda environment, use conda install jupyter notebook Jupyter notebooks are also available through pip with pip install jupyter notebook Markdown Cheatsheet : https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb Convert: https://nbconvert.readthedocs.io/en/latest/usage.html To create the slideshow from the notebook file, you'll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it. panda presentation: presentation","title":"Setting up Elasticsearch"},{"location":"setup/jupyter/#installing-jupyter-notebook","text":"By far the easiest way to install Jupyter is with Anaconda. Jupyter notebooks automatically come with the distribution. You'll be able to use notebooks from the default environment. To install Jupyter notebooks in a conda environment, use conda install jupyter notebook Jupyter notebooks are also available through pip with pip install jupyter notebook Markdown Cheatsheet : https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb Convert: https://nbconvert.readthedocs.io/en/latest/usage.html To create the slideshow from the notebook file, you'll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it. panda presentation: presentation","title":"Installing Jupyter Notebook"},{"location":"setup/lib/","text":"Python Libraries Following are the best Python Libraries: TensorFlow Scikit-Learn Numpy Keras PyTorch LightGBM Eli5 SciPy Theano Pandas","title":"Setting up Neo4J"},{"location":"setup/lib/#python-libraries","text":"Following are the best Python Libraries: TensorFlow Scikit-Learn Numpy Keras PyTorch LightGBM Eli5 SciPy Theano Pandas","title":"Python Libraries"}]}