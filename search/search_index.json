{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ICD 11 Code Tools This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Counting number of decendent nodes in each categories of ICD 11 code trees. Prepared By: Ping Lab Interns (UCLA)","title":"Home"},{"location":"#icd-11-code-tools","text":"This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Counting number of decendent nodes in each categories of ICD 11 code trees. Prepared By: Ping Lab Interns (UCLA)","title":"ICD 11 Code Tools"},{"location":"workflow/","text":"Cloud Articture and Workflow This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Platform architecture and workflow","title":"Workflow"},{"location":"workflow/#cloud-articture-and-workflow","text":"This online documents has been prepared for ICD 11 Code Tools development and implementation. The document begins with the setting up of tools and environment required for platform creation. A systematic steps for Indexing, Graph database and NoSQL database integration and development is presented. Implementation of AI search algorithms and development of models are introduced. Figure : Platform architecture and workflow","title":"Cloud Articture and Workflow"},{"location":"algorithms/algI/Algorithms-I/","text":"Algorithms - I Distance, Centrality, Community and Traversal import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( ignore ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G) 1. Distance Graph diameter, radius, eccentricity and other properties. center(G[, e, usebounds]) Returns the center of the graph G. diameter(G[, e, usebounds]) Returns the diameter of the graph G. eccentricity(G[, v, sp]) Returns the eccentricity of nodes in G. extrema_bounding(G[, compute]) Compute requested extreme distance metric of undirected graph G periphery(G[, e, usebounds]) Returns the periphery of the graph G. radius(G[, e, usebounds]) Returns the radius of the graph G. nx . radius(G), nx . diameter(G) (3, 5) nx . center(G) [0, 1, 2, 3, 8, 13, 19, 31] nx . eccentricity(G) {0: 3, 1: 3, 2: 3, 3: 3, 4: 4, 5: 4, 6: 4, 7: 4, 8: 3, 9: 4, 10: 4, 11: 4, 12: 4, 13: 3, 14: 5, 15: 5, 16: 5, 17: 4, 18: 5, 19: 3, 20: 5, 21: 4, 22: 5, 23: 5, 24: 4, 25: 4, 26: 5, 27: 4, 28: 4, 29: 5, 30: 4, 31: 3, 32: 4, 33: 4} nx . extrema_bounding(G) 5 nx . periphery(G) [14, 15, 16, 18, 20, 22, 23, 26, 29] Centrality Degree degree_centrality(G) Compute the degree centrality for nodes. in_degree_centrality(G) Compute the in-degree centrality for nodes. out_degree_centrality(G) Compute the out-degree centrality for nodes. G = nx . balanced_tree(r = 3 , h = 2 ) nx . draw(G, node_size = 500 ,node_color = lightblue , with_labels = True ) nx . degree_centrality(G) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . in_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . out_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} Eigenvector Centality eigenvector_centrality(G[, max_iter, tol, \u2026]) Compute the eigenvector centrality for the graph G. eigenvector_centrality_numpy(G[, weight, \u2026]) Compute the eigenvector centrality for the graph G. katz_centrality(G[, alpha, beta, max_iter, \u2026]) Compute the Katz centrality for the nodes of the graph G. katz_centrality_numpy(G[, alpha, beta, \u2026]) Compute the Katz centrality for the graph G. nx . eigenvector_centrality(G) {0: 0.49999972705204543, 1: 0.40824851115143324, 2: 0.40824851115143324, 3: 0.40824851115143324, 4: 0.16666657745857444, 5: 0.16666657745857444, 6: 0.16666657745857444, 7: 0.16666657745857444, 8: 0.16666657745857444, 9: 0.16666657745857444, 10: 0.16666657745857444, 11: 0.16666657745857444, 12: 0.16666657745857444} nx . katz_centrality(G) {0: 0.31855094383728, 1: 0.3279200842087277, 2: 0.3279200842087277, 3: 0.3279200842087277, 4: 0.2529669612371458, 5: 0.2529669612371458, 6: 0.2529669612371458, 7: 0.2529669612371458, 8: 0.2529669612371458, 9: 0.2529669612371458, 10: 0.2529669612371458, 11: 0.2529669612371458, 12: 0.2529669612371458} Betweenness Centrality betweenness_centrality(G[, k, normalized, \u2026]) Compute the shortest-path betweenness centrality for nodes. edge_betweenness_centrality(G[, k, \u2026]) Compute betweenness centrality for edges. betweenness_centrality_subset(G, sources, \u2026) Compute betweenness centrality for a subset of nodes. edge_betweenness_centrality_subset(G, \u2026[, \u2026]) Compute betweenness centrality for edges for a subset of nodes. nx . betweenness_centrality(G) {0: 0.7272727272727273, 1: 0.4545454545454546, 2: 0.4545454545454546, 3: 0.4545454545454546, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0} 2. Community Functions for computing and measuring community structure. The functions in this class are not imported into the top-level networkx namespace. You can access these functions by importing the networkx.algorithms.community module, then accessing the functions as attributes of community. For example: from networkx.algorithms import community G = nx . barbell_graph( 5 , 1 ) nx . draw(G) communities_generator = community . girvan_newman(G) top_level_communities = next (communities_generator) next_level_communities = next (communities_generator) sorted ( map ( sorted , next_level_communities)) [[0, 1, 2, 3, 4], [5], [6, 7, 8, 9, 10]] K-Clique Communty G = nx . relaxed_caveman_graph( 5 , 10 , 0.1 ) nx . draw(G, node_size = 40 ,node_color = blue ) from networkx.algorithms.community import k_clique_communities c = list (k_clique_communities(G,k = 4 )) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import greedy_modularity_communities c = list (greedy_modularity_communities(G)) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import asyn_lpa_communities c = list (asyn_lpa_communities(G)) print (c) [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}] from networkx.algorithms.community import label_propagation_communities c = list (label_propagation_communities(G)) print (c) [{10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}] from networkx.algorithms.community import asyn_fluidc c = list (asyn_fluidc(G,k = 5 )) print (c) [{16, 17, 19, 11}, {10, 12, 13, 14, 15, 18}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 40, 9, 8, 43, 44, 45, 46, 47, 48, 49, 41, 42}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}] 3 Traversal 3.1 Depth First Search Basic algorithms for depth-first searching the nodes of a graph. dfs_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS). dfs_tree(G[, source, depth_limit]) Returns oriented tree constructed from a depth-first-search from source. dfs_predecessors(G[, source, depth_limit]) Returns dictionary of predecessors in depth-first-search from source. dfs_successors(G[, source, depth_limit]) Returns dictionary of successors in depth-first-search from source. dfs_preorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search pre-ordering starting at source. dfs_postorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search post-ordering starting at source. dfs_labeled_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS) labeled by type. G = nx . random_tree( 20 ) nx . draw(G, node_size = 200 ,node_color = lightblue ,with_labels = True ) L = list (nx . dfs_edges(G, source = 0 , depth_limit = 5 )) print (L) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12)] TG = nx . dfs_tree(G) nx . draw(G, node_size = 200 ,node_color = lightblue ,with_labels = True ) nx . dfs_predecessors(G, source = 0 ) {1: 19, 2: 12, 3: 11, 4: 18, 5: 13, 6: 8, 7: 5, 8: 4, 9: 12, 10: 1, 11: 2, 12: 10, 13: 16, 14: 0, 15: 5, 16: 18, 17: 3, 18: 11, 19: 14} nx . dfs_successors(G, source = 0 ) {0: [14], 1: [10], 2: [11], 3: [17], 4: [8], 5: [7, 15], 8: [6], 10: [12], 11: [3, 18], 12: [9, 2], 13: [5], 14: [19], 16: [13], 18: [4, 16], 19: [1]} list (nx . dfs_preorder_nodes(G)) [0, 14, 19, 1, 10, 12, 9, 2, 11, 3, 17, 18, 4, 8, 6, 16, 13, 5, 7, 15] list (nx . dfs_postorder_nodes(G)) [9, 17, 3, 6, 8, 4, 7, 15, 5, 13, 16, 18, 11, 2, 12, 10, 1, 19, 14, 0] list (nx . dfs_labeled_edges(G)) [(0, 0, forward ), (0, 14, forward ), (14, 0, nontree ), (14, 19, forward ), (19, 14, nontree ), (19, 1, forward ), (1, 10, forward ), (10, 12, forward ), (12, 9, forward ), (9, 12, nontree ), (12, 9, reverse ), (12, 2, forward ), (2, 11, forward ), (11, 3, forward ), (3, 17, forward ), (17, 3, nontree ), (3, 17, reverse ), (3, 11, nontree ), (11, 3, reverse ), (11, 18, forward ), (18, 4, forward ), (4, 8, forward ), (8, 6, forward ), (6, 8, nontree ), (8, 6, reverse ), (8, 4, nontree ), (4, 8, reverse ), (4, 18, nontree ), (18, 4, reverse ), (18, 16, forward ), (16, 13, forward ), (13, 5, forward ), (5, 7, forward ), (7, 5, nontree ), (5, 7, reverse ), (5, 15, forward ), (15, 5, nontree ), (5, 15, reverse ), (5, 13, nontree ), (13, 5, reverse ), (13, 16, nontree ), (16, 13, reverse ), (16, 18, nontree ), (18, 16, reverse ), (18, 11, nontree ), (11, 18, reverse ), (11, 2, nontree ), (2, 11, reverse ), (2, 12, nontree ), (12, 2, reverse ), (12, 10, nontree ), (10, 12, reverse ), (10, 1, nontree ), (1, 10, reverse ), (1, 19, nontree ), (19, 1, reverse ), (14, 19, reverse ), (0, 14, reverse ), (0, 0, reverse )] 3.2 Breadth-first search Basic algorithms for breadth-first searching the nodes of a graph. bfs_edges(G, source[, reverse, depth_limit]) Iterate over edges in a breadth-first-search starting at source. bfs_tree(G, source[, reverse, depth_limit]) Returns an oriented tree constructed from of a breadth-first-search starting at source. bfs_predecessors(G, source[, depth_limit]) Returns an iterator of predecessors in breadth-first-search from source. bfs_successors(G, source[, depth_limit]) Returns an iterator of successors in breadth-first-search from so print ( list (nx . bfs_edges(G, 0 ))) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12), (12, 9), (12, 2), (2, 11), (11, 3), (11, 18), (3, 17), (18, 4), (18, 16), (4, 8), (16, 13), (8, 6), (13, 5), (5, 7), (5, 15)] nx . draw(nx . bfs_tree(G, 0 ), node_color = lightblue , node_size = 200 , with_labels = True ) print ( list (nx . bfs_predecessors(G, 0 ))) [(14, 0), (19, 14), (1, 19), (10, 1), (12, 10), (9, 12), (2, 12), (11, 2), (3, 11), (18, 11), (17, 3), (4, 18), (16, 18), (8, 4), (13, 16), (6, 8), (5, 13), (7, 5), (15, 5)] print ( list (nx . bfs_successors(G, 0 ))) [(0, [14]), (14, [19]), (19, [1]), (1, [10]), (10, [12]), (12, [9, 2]), (2, [11]), (11, [3, 18]), (3, [17]), (18, [4, 16]), (4, [8]), (16, [13]), (8, [6]), (13, [5]), (5, [7, 15])]","title":"Algorithms I"},{"location":"algorithms/algI/Algorithms-I/#algorithms-i","text":"","title":"Algorithms - I"},{"location":"algorithms/algI/Algorithms-I/#distance-centrality-community-and-traversal","text":"import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( ignore ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G)","title":"Distance, Centrality, Community and Traversal"},{"location":"algorithms/algI/Algorithms-I/#1-distance","text":"Graph diameter, radius, eccentricity and other properties. center(G[, e, usebounds]) Returns the center of the graph G. diameter(G[, e, usebounds]) Returns the diameter of the graph G. eccentricity(G[, v, sp]) Returns the eccentricity of nodes in G. extrema_bounding(G[, compute]) Compute requested extreme distance metric of undirected graph G periphery(G[, e, usebounds]) Returns the periphery of the graph G. radius(G[, e, usebounds]) Returns the radius of the graph G. nx . radius(G), nx . diameter(G) (3, 5) nx . center(G) [0, 1, 2, 3, 8, 13, 19, 31] nx . eccentricity(G) {0: 3, 1: 3, 2: 3, 3: 3, 4: 4, 5: 4, 6: 4, 7: 4, 8: 3, 9: 4, 10: 4, 11: 4, 12: 4, 13: 3, 14: 5, 15: 5, 16: 5, 17: 4, 18: 5, 19: 3, 20: 5, 21: 4, 22: 5, 23: 5, 24: 4, 25: 4, 26: 5, 27: 4, 28: 4, 29: 5, 30: 4, 31: 3, 32: 4, 33: 4} nx . extrema_bounding(G) 5 nx . periphery(G) [14, 15, 16, 18, 20, 22, 23, 26, 29]","title":"1. Distance"},{"location":"algorithms/algI/Algorithms-I/#centrality","text":"","title":"Centrality"},{"location":"algorithms/algI/Algorithms-I/#degree","text":"degree_centrality(G) Compute the degree centrality for nodes. in_degree_centrality(G) Compute the in-degree centrality for nodes. out_degree_centrality(G) Compute the out-degree centrality for nodes. G = nx . balanced_tree(r = 3 , h = 2 ) nx . draw(G, node_size = 500 ,node_color = lightblue , with_labels = True ) nx . degree_centrality(G) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . in_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333} nx . out_degree_centrality(nx . to_directed(G)) {0: 0.25, 1: 0.3333333333333333, 2: 0.3333333333333333, 3: 0.3333333333333333, 4: 0.08333333333333333, 5: 0.08333333333333333, 6: 0.08333333333333333, 7: 0.08333333333333333, 8: 0.08333333333333333, 9: 0.08333333333333333, 10: 0.08333333333333333, 11: 0.08333333333333333, 12: 0.08333333333333333}","title":"Degree"},{"location":"algorithms/algI/Algorithms-I/#eigenvector-centality","text":"eigenvector_centrality(G[, max_iter, tol, \u2026]) Compute the eigenvector centrality for the graph G. eigenvector_centrality_numpy(G[, weight, \u2026]) Compute the eigenvector centrality for the graph G. katz_centrality(G[, alpha, beta, max_iter, \u2026]) Compute the Katz centrality for the nodes of the graph G. katz_centrality_numpy(G[, alpha, beta, \u2026]) Compute the Katz centrality for the graph G. nx . eigenvector_centrality(G) {0: 0.49999972705204543, 1: 0.40824851115143324, 2: 0.40824851115143324, 3: 0.40824851115143324, 4: 0.16666657745857444, 5: 0.16666657745857444, 6: 0.16666657745857444, 7: 0.16666657745857444, 8: 0.16666657745857444, 9: 0.16666657745857444, 10: 0.16666657745857444, 11: 0.16666657745857444, 12: 0.16666657745857444} nx . katz_centrality(G) {0: 0.31855094383728, 1: 0.3279200842087277, 2: 0.3279200842087277, 3: 0.3279200842087277, 4: 0.2529669612371458, 5: 0.2529669612371458, 6: 0.2529669612371458, 7: 0.2529669612371458, 8: 0.2529669612371458, 9: 0.2529669612371458, 10: 0.2529669612371458, 11: 0.2529669612371458, 12: 0.2529669612371458}","title":"Eigenvector Centality"},{"location":"algorithms/algI/Algorithms-I/#betweenness-centrality","text":"betweenness_centrality(G[, k, normalized, \u2026]) Compute the shortest-path betweenness centrality for nodes. edge_betweenness_centrality(G[, k, \u2026]) Compute betweenness centrality for edges. betweenness_centrality_subset(G, sources, \u2026) Compute betweenness centrality for a subset of nodes. edge_betweenness_centrality_subset(G, \u2026[, \u2026]) Compute betweenness centrality for edges for a subset of nodes. nx . betweenness_centrality(G) {0: 0.7272727272727273, 1: 0.4545454545454546, 2: 0.4545454545454546, 3: 0.4545454545454546, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0}","title":"Betweenness Centrality"},{"location":"algorithms/algI/Algorithms-I/#2-community","text":"Functions for computing and measuring community structure. The functions in this class are not imported into the top-level networkx namespace. You can access these functions by importing the networkx.algorithms.community module, then accessing the functions as attributes of community. For example: from networkx.algorithms import community G = nx . barbell_graph( 5 , 1 ) nx . draw(G) communities_generator = community . girvan_newman(G) top_level_communities = next (communities_generator) next_level_communities = next (communities_generator) sorted ( map ( sorted , next_level_communities)) [[0, 1, 2, 3, 4], [5], [6, 7, 8, 9, 10]]","title":"2. Community"},{"location":"algorithms/algI/Algorithms-I/#k-clique-communty","text":"G = nx . relaxed_caveman_graph( 5 , 10 , 0.1 ) nx . draw(G, node_size = 40 ,node_color = blue ) from networkx.algorithms.community import k_clique_communities c = list (k_clique_communities(G,k = 4 )) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import greedy_modularity_communities c = list (greedy_modularity_communities(G)) print (c) [frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), frozenset({10, 11, 12, 13, 14, 15, 16, 17, 18, 19}), frozenset({20, 21, 22, 23, 24, 25, 26, 27, 28, 29}), frozenset({32, 33, 34, 35, 36, 37, 38, 39, 30, 31}), frozenset({40, 41, 42, 43, 44, 45, 46, 47, 48, 49})] from networkx.algorithms.community import asyn_lpa_communities c = list (asyn_lpa_communities(G)) print (c) [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}] from networkx.algorithms.community import label_propagation_communities c = list (label_propagation_communities(G)) print (c) [{10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {40, 41, 42, 43, 44, 45, 46, 47, 48, 49}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}] from networkx.algorithms.community import asyn_fluidc c = list (asyn_fluidc(G,k = 5 )) print (c) [{16, 17, 19, 11}, {10, 12, 13, 14, 15, 18}, {32, 33, 34, 35, 36, 37, 38, 39, 30, 31}, {0, 1, 2, 3, 4, 5, 6, 7, 40, 9, 8, 43, 44, 45, 46, 47, 48, 49, 41, 42}, {20, 21, 22, 23, 24, 25, 26, 27, 28, 29}]","title":"K-Clique Communty"},{"location":"algorithms/algI/Algorithms-I/#3-traversal","text":"","title":"3 Traversal"},{"location":"algorithms/algI/Algorithms-I/#31-depth-first-search","text":"Basic algorithms for depth-first searching the nodes of a graph. dfs_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS). dfs_tree(G[, source, depth_limit]) Returns oriented tree constructed from a depth-first-search from source. dfs_predecessors(G[, source, depth_limit]) Returns dictionary of predecessors in depth-first-search from source. dfs_successors(G[, source, depth_limit]) Returns dictionary of successors in depth-first-search from source. dfs_preorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search pre-ordering starting at source. dfs_postorder_nodes(G[, source, depth_limit]) Generate nodes in a depth-first-search post-ordering starting at source. dfs_labeled_edges(G[, source, depth_limit]) Iterate over edges in a depth-first-search (DFS) labeled by type. G = nx . random_tree( 20 ) nx . draw(G, node_size = 200 ,node_color = lightblue ,with_labels = True ) L = list (nx . dfs_edges(G, source = 0 , depth_limit = 5 )) print (L) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12)] TG = nx . dfs_tree(G) nx . draw(G, node_size = 200 ,node_color = lightblue ,with_labels = True ) nx . dfs_predecessors(G, source = 0 ) {1: 19, 2: 12, 3: 11, 4: 18, 5: 13, 6: 8, 7: 5, 8: 4, 9: 12, 10: 1, 11: 2, 12: 10, 13: 16, 14: 0, 15: 5, 16: 18, 17: 3, 18: 11, 19: 14} nx . dfs_successors(G, source = 0 ) {0: [14], 1: [10], 2: [11], 3: [17], 4: [8], 5: [7, 15], 8: [6], 10: [12], 11: [3, 18], 12: [9, 2], 13: [5], 14: [19], 16: [13], 18: [4, 16], 19: [1]} list (nx . dfs_preorder_nodes(G)) [0, 14, 19, 1, 10, 12, 9, 2, 11, 3, 17, 18, 4, 8, 6, 16, 13, 5, 7, 15] list (nx . dfs_postorder_nodes(G)) [9, 17, 3, 6, 8, 4, 7, 15, 5, 13, 16, 18, 11, 2, 12, 10, 1, 19, 14, 0] list (nx . dfs_labeled_edges(G)) [(0, 0, forward ), (0, 14, forward ), (14, 0, nontree ), (14, 19, forward ), (19, 14, nontree ), (19, 1, forward ), (1, 10, forward ), (10, 12, forward ), (12, 9, forward ), (9, 12, nontree ), (12, 9, reverse ), (12, 2, forward ), (2, 11, forward ), (11, 3, forward ), (3, 17, forward ), (17, 3, nontree ), (3, 17, reverse ), (3, 11, nontree ), (11, 3, reverse ), (11, 18, forward ), (18, 4, forward ), (4, 8, forward ), (8, 6, forward ), (6, 8, nontree ), (8, 6, reverse ), (8, 4, nontree ), (4, 8, reverse ), (4, 18, nontree ), (18, 4, reverse ), (18, 16, forward ), (16, 13, forward ), (13, 5, forward ), (5, 7, forward ), (7, 5, nontree ), (5, 7, reverse ), (5, 15, forward ), (15, 5, nontree ), (5, 15, reverse ), (5, 13, nontree ), (13, 5, reverse ), (13, 16, nontree ), (16, 13, reverse ), (16, 18, nontree ), (18, 16, reverse ), (18, 11, nontree ), (11, 18, reverse ), (11, 2, nontree ), (2, 11, reverse ), (2, 12, nontree ), (12, 2, reverse ), (12, 10, nontree ), (10, 12, reverse ), (10, 1, nontree ), (1, 10, reverse ), (1, 19, nontree ), (19, 1, reverse ), (14, 19, reverse ), (0, 14, reverse ), (0, 0, reverse )]","title":"3.1 Depth First Search"},{"location":"algorithms/algI/Algorithms-I/#32-breadth-first-search","text":"Basic algorithms for breadth-first searching the nodes of a graph. bfs_edges(G, source[, reverse, depth_limit]) Iterate over edges in a breadth-first-search starting at source. bfs_tree(G, source[, reverse, depth_limit]) Returns an oriented tree constructed from of a breadth-first-search starting at source. bfs_predecessors(G, source[, depth_limit]) Returns an iterator of predecessors in breadth-first-search from source. bfs_successors(G, source[, depth_limit]) Returns an iterator of successors in breadth-first-search from so print ( list (nx . bfs_edges(G, 0 ))) [(0, 14), (14, 19), (19, 1), (1, 10), (10, 12), (12, 9), (12, 2), (2, 11), (11, 3), (11, 18), (3, 17), (18, 4), (18, 16), (4, 8), (16, 13), (8, 6), (13, 5), (5, 7), (5, 15)] nx . draw(nx . bfs_tree(G, 0 ), node_color = lightblue , node_size = 200 , with_labels = True ) print ( list (nx . bfs_predecessors(G, 0 ))) [(14, 0), (19, 14), (1, 19), (10, 1), (12, 10), (9, 12), (2, 12), (11, 2), (3, 11), (18, 11), (17, 3), (4, 18), (16, 18), (8, 4), (13, 16), (6, 8), (5, 13), (7, 5), (15, 5)] print ( list (nx . bfs_successors(G, 0 ))) [(0, [14]), (14, [19]), (19, [1]), (1, [10]), (10, [12]), (12, [9, 2]), (2, [11]), (11, [3, 18]), (3, [17]), (18, [4, 16]), (4, [8]), (16, [13]), (8, [6]), (13, [5]), (5, [7, 15])]","title":"3.2 Breadth-first search"},{"location":"algorithms/algII/Algorithms-II/","text":"Algorithm - II Clustering, Link Analysis, Node Classification, Link Prediction import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( ignore ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True ) Clustering Algorithms to characterize the number of triangles in a graph. triangles(G[, nodes]) Compute the number of triangles. transitivity(G) Compute graph transitivity, the fraction of all possible triangles present in G. clustering(G[, nodes, weight]) Compute the clustering coefficient for nodes. average_clustering(G[, nodes, weight, \u2026]) Compute the average clustering coefficient for the graph G. square_clustering(G[, nodes]) Compute the squares clustering coefficient for nodes. generalized_degree(G[, nodes]) Compute the generalized degree for nodes. nx . triangles(G) {0: 18, 1: 12, 2: 11, 3: 10, 4: 2, 5: 3, 6: 3, 7: 6, 8: 5, 9: 0, 10: 2, 11: 0, 12: 1, 13: 6, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 4, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 4, 30: 3, 31: 3, 32: 13, 33: 15} nx . transitivity(G) 0.2556818181818182 nx . clustering(G) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} Link Analysis PageRank PageRank analysis of graph structure. pagerank(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_numpy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_scipy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. google_matrix(G[, alpha, personalization, \u2026]) Returns the Google matrix of the graph. nx . pagerank(G) {0: 0.09700181758983709, 1: 0.05287839103742701, 2: 0.057078423047636745, 3: 0.03586064322306479, 4: 0.021979406974834498, 5: 0.02911334166344221, 6: 0.02911334166344221, 7: 0.024490758039509182, 8: 0.029765339186167028, 9: 0.014308950284462801, 10: 0.021979406974834498, 11: 0.009564916863537148, 12: 0.014645186487916191, 13: 0.029536314977202986, 14: 0.014535161524273825, 15: 0.014535161524273825, 16: 0.016785378110253487, 17: 0.014558859774243493, 18: 0.014535161524273825, 19: 0.019604416711937293, 20: 0.014535161524273825, 21: 0.014558859774243493, 22: 0.014535161524273825, 23: 0.03152091531163228, 24: 0.021075455001162945, 25: 0.021005628174745786, 26: 0.015043395360629753, 27: 0.025638803528350497, 28: 0.01957296050943854, 29: 0.02628726283711208, 30: 0.02458933653429248, 31: 0.03715663592267942, 32: 0.07169213006588289, 33: 0.1009179167487121} nx . google_matrix(G) matrix([[0.00441176, 0.05753676, 0.05753676, ..., 0.05753676, 0.00441176, 0.00441176], [0.09885621, 0.00441176, 0.09885621, ..., 0.00441176, 0.00441176, 0.00441176], [0.08941176, 0.08941176, 0.00441176, ..., 0.00441176, 0.08941176, 0.00441176], ..., [0.14607843, 0.00441176, 0.00441176, ..., 0.00441176, 0.14607843, 0.14607843], [0.00441176, 0.00441176, 0.0752451 , ..., 0.0752451 , 0.00441176, 0.0752451 ], [0.00441176, 0.00441176, 0.00441176, ..., 0.05441176, 0.05441176, 0.00441176]]) Hits Hubs and authorities analysis of graph structure. hits(G[, max_iter, tol, nstart, normalized]) Returns HITS hubs and authorities values for nodes. hits_numpy(G[, normalized]) Returns HITS hubs and authorities values for nodes. hits_scipy(G[, max_iter, tol, normalized]) Returns HITS hubs and authorities values for nodes. hub_matrix(G[, nodelist]) Returns the HITS hub matrix. authority_matrix(G[, nodelist]) Returns the HITS authority matrix. nx . hits(G) ({0: 0.07141272875773573, 1: 0.053427231205172614, 2: 0.06371906453963268, 3: 0.04242273710428976, 4: 0.01526095969815266, 5: 0.015966913494418547, 6: 0.015966913494418547, 7: 0.034343167206797434, 8: 0.0456819251308063, 9: 0.020625667757182626, 10: 0.01526095969815266, 11: 0.01061789150852051, 12: 0.01692545078543599, 13: 0.04549486406600547, 14: 0.020370345842716076, 15: 0.020370345842716076, 16: 0.004748031841562519, 17: 0.018561637031907358, 18: 0.020370345842716076, 19: 0.02971333389111539, 20: 0.020370345842716076, 21: 0.018561637031907358, 22: 0.020370345842716076, 23: 0.030156497528902444, 24: 0.011460952230139869, 25: 0.01189366439609368, 26: 0.015182734341447207, 27: 0.02681349412708363, 28: 0.0263315057833753, 29: 0.027111539646424865, 30: 0.03510623798827733, 31: 0.03837574188047834, 32: 0.06200184647463986, 33: 0.07500294214634279}, {0: 0.07141272880870855, 1: 0.05342723122870397, 2: 0.06371906455587135, 3: 0.04242273710611524, 4: 0.015260959692251741, 5: 0.01596691348769785, 6: 0.01596691348769785, 7: 0.03434316719678568, 8: 0.045681925113766106, 9: 0.020625667747004237, 10: 0.015260959692251741, 11: 0.010617891499780771, 12: 0.016925450777611116, 13: 0.045494864044925934, 14: 0.02037034582705704, 15: 0.02037034582705704, 16: 0.004748031844529441, 17: 0.01856163702009135, 18: 0.02037034582705704, 19: 0.029713333868231606, 20: 0.02037034582705704, 21: 0.01856163702009135, 22: 0.02037034582705704, 23: 0.030156497522138854, 24: 0.011460952243147787, 25: 0.011893664411194165, 26: 0.015182734336172116, 27: 0.026813494122100573, 28: 0.026331505783102067, 29: 0.02711153964098065, 30: 0.03510623797808329, 31: 0.03837574185646307, 32: 0.06200184653550559, 33: 0.0750029422437107}) nx . hub_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]]) nx . authority_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]]) Node Classification This module provides the functions for node classification problem. The functions in this module are not imported into the top level networkx namespace. You can access these functions by importing the networkx.algorithms.node_classification modules, then accessing the functions as attributes of node_classification. For example: import networkx as nx from networkx.algorithms import node_classification G = nx . balanced_tree( 3 , 3 ) nx . draw(G, node_size = 500 , node_color = lightgreen , with_labels = True ) G . node[ 1 ][ label ] = A G . node[ 2 ][ label ] = B G . node[ 3 ][ label ] = C L = node_classification . harmonic_function(G) print (L) [ A , A , B , C , A , A , A , B , B , B , C , C , C , A , A , A , A , A , A , A , A , A , B , B , B , B , B , B , B , B , B , C , C , C , C , C , C , C , C , C ] LL = {} for n,l in zip (G . nodes(),L): LL . update({n:l}) nx . draw(G, node_size = 500 , labels = LL, node_color = lightgreen , with_labels = True ) Link Prediction Link prediction algorithms. resource_allocation_index(G[, ebunch]) Compute the resource allocation index of all node pairs in ebunch. jaccard_coefficient(G[, ebunch]) Compute the Jaccard coefficient of all node pairs in ebunch. adamic_adar_index(G[, ebunch]) Compute the Adamic-Adar index of all node pairs in ebunch. preferential_attachment(G[, ebunch]) Compute the preferential attachment score of all node pairs in ebunch. cn_soundarajan_hopcroft(G[, ebunch, community]) Count the number of common neighbors of all node pairs in ebunch ra_index_soundarajan_hopcroft(G[, ebunch, \u2026]) Compute the resource allocation index of all node pairs in ebunch using community information. within_inter_cluster(G[, ebunch, delta, \u2026]) Compute the ratio of within- and inter-cluster common neighb G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True ) preds = nx . resource_allocation_index(G, [( 0 , 10 ),( 9 , 18 ), ( 11 , 12 ),( 30 , 27 ),( 16 , 26 )]) for u, v, p in preds: print ( ( %d , %d ) - %.8f % (u, v, p)) (0, 10) - 0.58333333 (9, 18) - 0.05882353 (11, 12) - 0.06250000 (30, 27) - 0.05882353 (16, 26) - 0.00000000","title":"Algorithms II"},{"location":"algorithms/algII/Algorithms-II/#algorithm-ii","text":"","title":"Algorithm - II"},{"location":"algorithms/algII/Algorithms-II/#clustering-link-analysis-node-classification-link-prediction","text":"import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( ignore ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True )","title":"Clustering, Link Analysis, Node Classification, Link Prediction"},{"location":"algorithms/algII/Algorithms-II/#clustering","text":"Algorithms to characterize the number of triangles in a graph. triangles(G[, nodes]) Compute the number of triangles. transitivity(G) Compute graph transitivity, the fraction of all possible triangles present in G. clustering(G[, nodes, weight]) Compute the clustering coefficient for nodes. average_clustering(G[, nodes, weight, \u2026]) Compute the average clustering coefficient for the graph G. square_clustering(G[, nodes]) Compute the squares clustering coefficient for nodes. generalized_degree(G[, nodes]) Compute the generalized degree for nodes. nx . triangles(G) {0: 18, 1: 12, 2: 11, 3: 10, 4: 2, 5: 3, 6: 3, 7: 6, 8: 5, 9: 0, 10: 2, 11: 0, 12: 1, 13: 6, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 4, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 4, 30: 3, 31: 3, 32: 13, 33: 15} nx . transitivity(G) 0.2556818181818182 nx . clustering(G) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882}","title":"Clustering"},{"location":"algorithms/algII/Algorithms-II/#link-analysis","text":"","title":"Link Analysis"},{"location":"algorithms/algII/Algorithms-II/#pagerank","text":"PageRank analysis of graph structure. pagerank(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_numpy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. pagerank_scipy(G[, alpha, personalization, \u2026]) Returns the PageRank of the nodes in the graph. google_matrix(G[, alpha, personalization, \u2026]) Returns the Google matrix of the graph. nx . pagerank(G) {0: 0.09700181758983709, 1: 0.05287839103742701, 2: 0.057078423047636745, 3: 0.03586064322306479, 4: 0.021979406974834498, 5: 0.02911334166344221, 6: 0.02911334166344221, 7: 0.024490758039509182, 8: 0.029765339186167028, 9: 0.014308950284462801, 10: 0.021979406974834498, 11: 0.009564916863537148, 12: 0.014645186487916191, 13: 0.029536314977202986, 14: 0.014535161524273825, 15: 0.014535161524273825, 16: 0.016785378110253487, 17: 0.014558859774243493, 18: 0.014535161524273825, 19: 0.019604416711937293, 20: 0.014535161524273825, 21: 0.014558859774243493, 22: 0.014535161524273825, 23: 0.03152091531163228, 24: 0.021075455001162945, 25: 0.021005628174745786, 26: 0.015043395360629753, 27: 0.025638803528350497, 28: 0.01957296050943854, 29: 0.02628726283711208, 30: 0.02458933653429248, 31: 0.03715663592267942, 32: 0.07169213006588289, 33: 0.1009179167487121} nx . google_matrix(G) matrix([[0.00441176, 0.05753676, 0.05753676, ..., 0.05753676, 0.00441176, 0.00441176], [0.09885621, 0.00441176, 0.09885621, ..., 0.00441176, 0.00441176, 0.00441176], [0.08941176, 0.08941176, 0.00441176, ..., 0.00441176, 0.08941176, 0.00441176], ..., [0.14607843, 0.00441176, 0.00441176, ..., 0.00441176, 0.14607843, 0.14607843], [0.00441176, 0.00441176, 0.0752451 , ..., 0.0752451 , 0.00441176, 0.0752451 ], [0.00441176, 0.00441176, 0.00441176, ..., 0.05441176, 0.05441176, 0.00441176]])","title":"PageRank"},{"location":"algorithms/algII/Algorithms-II/#hits","text":"Hubs and authorities analysis of graph structure. hits(G[, max_iter, tol, nstart, normalized]) Returns HITS hubs and authorities values for nodes. hits_numpy(G[, normalized]) Returns HITS hubs and authorities values for nodes. hits_scipy(G[, max_iter, tol, normalized]) Returns HITS hubs and authorities values for nodes. hub_matrix(G[, nodelist]) Returns the HITS hub matrix. authority_matrix(G[, nodelist]) Returns the HITS authority matrix. nx . hits(G) ({0: 0.07141272875773573, 1: 0.053427231205172614, 2: 0.06371906453963268, 3: 0.04242273710428976, 4: 0.01526095969815266, 5: 0.015966913494418547, 6: 0.015966913494418547, 7: 0.034343167206797434, 8: 0.0456819251308063, 9: 0.020625667757182626, 10: 0.01526095969815266, 11: 0.01061789150852051, 12: 0.01692545078543599, 13: 0.04549486406600547, 14: 0.020370345842716076, 15: 0.020370345842716076, 16: 0.004748031841562519, 17: 0.018561637031907358, 18: 0.020370345842716076, 19: 0.02971333389111539, 20: 0.020370345842716076, 21: 0.018561637031907358, 22: 0.020370345842716076, 23: 0.030156497528902444, 24: 0.011460952230139869, 25: 0.01189366439609368, 26: 0.015182734341447207, 27: 0.02681349412708363, 28: 0.0263315057833753, 29: 0.027111539646424865, 30: 0.03510623798827733, 31: 0.03837574188047834, 32: 0.06200184647463986, 33: 0.07500294214634279}, {0: 0.07141272880870855, 1: 0.05342723122870397, 2: 0.06371906455587135, 3: 0.04242273710611524, 4: 0.015260959692251741, 5: 0.01596691348769785, 6: 0.01596691348769785, 7: 0.03434316719678568, 8: 0.045681925113766106, 9: 0.020625667747004237, 10: 0.015260959692251741, 11: 0.010617891499780771, 12: 0.016925450777611116, 13: 0.045494864044925934, 14: 0.02037034582705704, 15: 0.02037034582705704, 16: 0.004748031844529441, 17: 0.01856163702009135, 18: 0.02037034582705704, 19: 0.029713333868231606, 20: 0.02037034582705704, 21: 0.01856163702009135, 22: 0.02037034582705704, 23: 0.030156497522138854, 24: 0.011460952243147787, 25: 0.011893664411194165, 26: 0.015182734336172116, 27: 0.026813494122100573, 28: 0.026331505783102067, 29: 0.02711153964098065, 30: 0.03510623797808329, 31: 0.03837574185646307, 32: 0.06200184653550559, 33: 0.0750029422437107}) nx . hub_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]]) nx . authority_matrix(G) matrix([[16., 7., 5., ..., 0., 3., 4.], [ 7., 9., 4., ..., 1., 2., 3.], [ 5., 4., 10., ..., 3., 1., 6.], ..., [ 0., 1., 3., ..., 6., 1., 2.], [ 3., 2., 1., ..., 1., 12., 10.], [ 4., 3., 6., ..., 2., 10., 17.]])","title":"Hits"},{"location":"algorithms/algII/Algorithms-II/#node-classification","text":"This module provides the functions for node classification problem. The functions in this module are not imported into the top level networkx namespace. You can access these functions by importing the networkx.algorithms.node_classification modules, then accessing the functions as attributes of node_classification. For example: import networkx as nx from networkx.algorithms import node_classification G = nx . balanced_tree( 3 , 3 ) nx . draw(G, node_size = 500 , node_color = lightgreen , with_labels = True ) G . node[ 1 ][ label ] = A G . node[ 2 ][ label ] = B G . node[ 3 ][ label ] = C L = node_classification . harmonic_function(G) print (L) [ A , A , B , C , A , A , A , B , B , B , C , C , C , A , A , A , A , A , A , A , A , A , B , B , B , B , B , B , B , B , B , C , C , C , C , C , C , C , C , C ] LL = {} for n,l in zip (G . nodes(),L): LL . update({n:l}) nx . draw(G, node_size = 500 , labels = LL, node_color = lightgreen , with_labels = True )","title":"Node Classification"},{"location":"algorithms/algII/Algorithms-II/#link-prediction","text":"Link prediction algorithms. resource_allocation_index(G[, ebunch]) Compute the resource allocation index of all node pairs in ebunch. jaccard_coefficient(G[, ebunch]) Compute the Jaccard coefficient of all node pairs in ebunch. adamic_adar_index(G[, ebunch]) Compute the Adamic-Adar index of all node pairs in ebunch. preferential_attachment(G[, ebunch]) Compute the preferential attachment score of all node pairs in ebunch. cn_soundarajan_hopcroft(G[, ebunch, community]) Count the number of common neighbors of all node pairs in ebunch ra_index_soundarajan_hopcroft(G[, ebunch, \u2026]) Compute the resource allocation index of all node pairs in ebunch using community information. within_inter_cluster(G[, ebunch, delta, \u2026]) Compute the ratio of within- and inter-cluster common neighb G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True ) preds = nx . resource_allocation_index(G, [( 0 , 10 ),( 9 , 18 ), ( 11 , 12 ),( 30 , 27 ),( 16 , 26 )]) for u, v, p in preds: print ( ( %d , %d ) - %.8f % (u, v, p)) (0, 10) - 0.58333333 (9, 18) - 0.05882353 (11, 12) - 0.06250000 (30, 27) - 0.05882353 (16, 26) - 0.00000000","title":"Link Prediction"},{"location":"algorithms/algIII/Algorithms-III/","text":"Algorithm - III Operators, Shortest Paths, Trees, Planarity, Flows, Directed Acyclic Graphs, Approximations and Heuristics, Assortativity import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( ignore ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True ) Operators Unary operations on graphs complement(G) Returns the graph complement of G. reverse(G[, copy]) Returns the reverse directed graph of G. Operations on graphs including union, intersection, difference. compose(G, H) Returns a new graph of G composed with H. union(G, H[, rename, name]) Return the union of graphs G and H. disjoint_union(G, H) Return the disjoint union of graphs G and H. intersection(G, H) Returns a new graph that contains only the edges that exist in both G and H. difference(G, H) Returns a new graph that contains the edges that exist in G but not in H. symmetric_difference(G, H) Returns new graph with edges that exist in either G or H but not both. Operations on many graphs. compose_all(graphs) Returns the composition of all graphs. union_all(graphs[, rename]) Returns the union of all graphs. disjoint_union_all(graphs) Returns the disjoint union of all graphs. intersection_all(graphs) Returns a new graph that contains only the edges that exist in all graphs. Graph products. cartesian_product(G, H) Returns the Cartesian product of G and H. lexicographic_product(G, H) Returns the lexicographic product of G and H. rooted_product(G, H, root) Return the rooted product of graphs G and H rooted at root in H. strong_product(G, H) Returns the strong product of G and H. tensor_product(G, H) Returns the tensor product of G and H. power(G, k) Returns the specified power of a graph. CG = nx . complement(G) nx . draw(CG, node_size = 500 , node_color = lightblue , with_labels = True ) RG = nx . reverse(nx . to_directed(G)) nx . draw(RG, node_size = 500 , node_color = lightblue , with_labels = True ) G = nx . balanced_tree( 2 , 3 ) H = nx . balanced_tree( 3 , 2 ) plt . figure(figsize = [ 20 , 8 ]) plt . subplot( 1 , 3 , 1 ) nx . draw(G, node_size = 500 ,node_color = lightblue , with_labels = True ) plt . subplot( 1 , 3 , 2 ) nx . draw(G, node_size = 500 ,node_color = lightgreen , with_labels = True ) plt . subplot( 1 , 3 , 3 ) nx . draw(nx . compose(H,G), node_size = 500 , node_color = magenta , with_labels = True ) G = nx . balanced_tree( 3 , 2 ) nx . draw(nx . power(G,k = 3 ), node_size = 500 , node_color = magenta , with_labels = True ) Shortest Paths Compute the shortest paths and path lengths between nodes in the graph. These algorithms work with undirected and directed graphs. shortest_path(G[, source, target, weight, \u2026]) Compute shortest paths in the graph. all_shortest_paths(G, source, target[, \u2026]) Compute all shortest paths in the graph. shortest_path_length(G[, source, target, \u2026]) Compute shortest path lengths in the graph. average_shortest_path_length(G[, weight, method]) Returns the average shortest path length. has_path(G, source, target) Returns True if G has a path from source to target. Advanced Interface Shortest path algorithms for unweighted graphs. single_source_shortest_path(G, source[, cutoff]) Compute shortest path between source and all other nodes reachable from source. single_source_shortest_path_length(G, source) Compute the shortest path lengths from source to all reachable nodes. single_target_shortest_path(G, target[, cutoff]) Compute shortest path to target from all nodes that reach target. single_target_shortest_path_length(G, target) Compute the shortest path lengths to target from all reachable nodes. bidirectional_shortest_path(G, source, target) Returns a list of nodes in a shortest path between source and target. all_pairs_shortest_path(G[, cutoff]) Compute shortest paths between all nodes. all_pairs_shortest_path_length(G[, cutoff]) Computes the shortest path lengths between all nodes in G. predecessor(G, source[, target, cutoff, \u2026]) Returns dict of predecessors for the path from source to all nodes in G Shortest path algorithms for weighed graphs. dijkstra_predecessor_and_distance(G, source) Compute weighted shortest path length and predecessors. dijkstra_path(G, source, target[, weight]) Returns the shortest weighted path from source to target in G. dijkstra_path_length(G, source, target[, weight]) Returns the shortest weighted path length in G from source to target. single_source_dijkstra(G, source[, target, \u2026]) Find shortest weighted paths and lengths from a source node. single_source_dijkstra_path(G, source[, \u2026]) Find shortest weighted paths in G from a source node. single_source_dijkstra_path_length(G, source) Find shortest weighted path lengths in G from a source node. multi_source_dijkstra(G, sources[, target, \u2026]) Find shortest weighted paths and lengths from a given set of source nodes. multi_source_dijkstra_path(G, sources[, \u2026]) Find shortest weighted paths in G from a given set of source nodes. multi_source_dijkstra_path_length(G, sources) Find shortest weighted path lengths in G from a given set of source nodes. all_pairs_dijkstra(G[, cutoff, weight]) Find shortest weighted paths and lengths between all nodes. all_pairs_dijkstra_path(G[, cutoff, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_dijkstra_path_length(G[, cutoff, \u2026]) Compute shortest path lengths between all nodes in a weighted graph. bidirectional_dijkstra(G, source, target[, \u2026]) Dijkstra\u2019s algorithm for shortest paths using bidirectional search. bellman_ford_path(G, source, target[, weight]) Returns the shortest path from source to target in a weighted graph G. bellman_ford_path_length(G, source, target) Returns the shortest path length from source to target in a weighted graph. single_source_bellman_ford(G, source[, \u2026]) Compute shortest paths and lengths in a weighted graph G. single_source_bellman_ford_path(G, source[, \u2026]) Compute shortest path between source and all other reachable nodes for a weighted graph. single_source_bellman_ford_path_length(G, source) Compute the shortest path length between source and all other reachable nodes for a weighted graph. all_pairs_bellman_ford_path(G[, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_bellman_ford_path_length(G[, weight]) Compute shortest path lengths between all nodes in a weighted graph. bellman_ford_predecessor_and_distance(G, source) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. negative_edge_cycle(G[, weight]) Returns True if there exists a negative edge cycle anywhere in G. goldberg_radzik(G, source[, weight]) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. johnson(G[, weight]) Uses Johnson\u2019s Algorithm to compute shortest paths. Dense Graphs Floyd-Warshall algorithm for shortest paths. floyd_warshall(G[, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_predecessor_and_distance(G[, \u2026]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_numpy(G[, nodelist, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. reconstruct_path(source, target, predecessors) Reconstruct a path from source to target using the predecessors dict as returned by floyd_warshall_predecessor_and_distance A* Algorithm Shortest paths and path lengths using the A* (\u201cA star\u201d) algorithm. astar_path(G, source, target[, heuristic, \u2026]) Returns a list of nodes in a shortest path between source and target using the A* (\u201cA-star\u201d) algorithm. astar_path_length(G, source, target[, \u2026]) Returns the length of the shortest path between source and target using t G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True ) nx . shortest_path(G, 0 , 26 ) [0, 8, 33, 26] list (nx . all_shortest_paths(G, 0 , 26 )) [[0, 8, 33, 26], [0, 13, 33, 26], [0, 19, 33, 26], [0, 31, 33, 26]]","title":"Algorithms III"},{"location":"algorithms/algIII/Algorithms-III/#algorithm-iii","text":"","title":"Algorithm - III"},{"location":"algorithms/algIII/Algorithms-III/#operators-shortest-paths-trees-planarity-flows-directed-acyclic-graphs-approximations-and-heuristics-assortativity","text":"import matplotlib.pyplot as plt import networkx as nx import seaborn as sns sns . set() % matplotlib inline import warnings import matplotlib.cbook warnings . filterwarnings( ignore ,category = matplotlib . cbook . mplDeprecation) G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True )","title":"Operators, Shortest Paths, Trees, Planarity, Flows, Directed Acyclic Graphs, Approximations and Heuristics,  Assortativity"},{"location":"algorithms/algIII/Algorithms-III/#operators","text":"Unary operations on graphs complement(G) Returns the graph complement of G. reverse(G[, copy]) Returns the reverse directed graph of G. Operations on graphs including union, intersection, difference. compose(G, H) Returns a new graph of G composed with H. union(G, H[, rename, name]) Return the union of graphs G and H. disjoint_union(G, H) Return the disjoint union of graphs G and H. intersection(G, H) Returns a new graph that contains only the edges that exist in both G and H. difference(G, H) Returns a new graph that contains the edges that exist in G but not in H. symmetric_difference(G, H) Returns new graph with edges that exist in either G or H but not both. Operations on many graphs. compose_all(graphs) Returns the composition of all graphs. union_all(graphs[, rename]) Returns the union of all graphs. disjoint_union_all(graphs) Returns the disjoint union of all graphs. intersection_all(graphs) Returns a new graph that contains only the edges that exist in all graphs. Graph products. cartesian_product(G, H) Returns the Cartesian product of G and H. lexicographic_product(G, H) Returns the lexicographic product of G and H. rooted_product(G, H, root) Return the rooted product of graphs G and H rooted at root in H. strong_product(G, H) Returns the strong product of G and H. tensor_product(G, H) Returns the tensor product of G and H. power(G, k) Returns the specified power of a graph. CG = nx . complement(G) nx . draw(CG, node_size = 500 , node_color = lightblue , with_labels = True ) RG = nx . reverse(nx . to_directed(G)) nx . draw(RG, node_size = 500 , node_color = lightblue , with_labels = True ) G = nx . balanced_tree( 2 , 3 ) H = nx . balanced_tree( 3 , 2 ) plt . figure(figsize = [ 20 , 8 ]) plt . subplot( 1 , 3 , 1 ) nx . draw(G, node_size = 500 ,node_color = lightblue , with_labels = True ) plt . subplot( 1 , 3 , 2 ) nx . draw(G, node_size = 500 ,node_color = lightgreen , with_labels = True ) plt . subplot( 1 , 3 , 3 ) nx . draw(nx . compose(H,G), node_size = 500 , node_color = magenta , with_labels = True ) G = nx . balanced_tree( 3 , 2 ) nx . draw(nx . power(G,k = 3 ), node_size = 500 , node_color = magenta , with_labels = True )","title":"Operators"},{"location":"algorithms/algIII/Algorithms-III/#shortest-paths","text":"Compute the shortest paths and path lengths between nodes in the graph. These algorithms work with undirected and directed graphs. shortest_path(G[, source, target, weight, \u2026]) Compute shortest paths in the graph. all_shortest_paths(G, source, target[, \u2026]) Compute all shortest paths in the graph. shortest_path_length(G[, source, target, \u2026]) Compute shortest path lengths in the graph. average_shortest_path_length(G[, weight, method]) Returns the average shortest path length. has_path(G, source, target) Returns True if G has a path from source to target. Advanced Interface","title":"Shortest Paths"},{"location":"algorithms/algIII/Algorithms-III/#shortest-path-algorithms-for-unweighted-graphs","text":"single_source_shortest_path(G, source[, cutoff]) Compute shortest path between source and all other nodes reachable from source. single_source_shortest_path_length(G, source) Compute the shortest path lengths from source to all reachable nodes. single_target_shortest_path(G, target[, cutoff]) Compute shortest path to target from all nodes that reach target. single_target_shortest_path_length(G, target) Compute the shortest path lengths to target from all reachable nodes. bidirectional_shortest_path(G, source, target) Returns a list of nodes in a shortest path between source and target. all_pairs_shortest_path(G[, cutoff]) Compute shortest paths between all nodes. all_pairs_shortest_path_length(G[, cutoff]) Computes the shortest path lengths between all nodes in G. predecessor(G, source[, target, cutoff, \u2026]) Returns dict of predecessors for the path from source to all nodes in G","title":"Shortest path algorithms for unweighted graphs."},{"location":"algorithms/algIII/Algorithms-III/#shortest-path-algorithms-for-weighed-graphs","text":"dijkstra_predecessor_and_distance(G, source) Compute weighted shortest path length and predecessors. dijkstra_path(G, source, target[, weight]) Returns the shortest weighted path from source to target in G. dijkstra_path_length(G, source, target[, weight]) Returns the shortest weighted path length in G from source to target. single_source_dijkstra(G, source[, target, \u2026]) Find shortest weighted paths and lengths from a source node. single_source_dijkstra_path(G, source[, \u2026]) Find shortest weighted paths in G from a source node. single_source_dijkstra_path_length(G, source) Find shortest weighted path lengths in G from a source node. multi_source_dijkstra(G, sources[, target, \u2026]) Find shortest weighted paths and lengths from a given set of source nodes. multi_source_dijkstra_path(G, sources[, \u2026]) Find shortest weighted paths in G from a given set of source nodes. multi_source_dijkstra_path_length(G, sources) Find shortest weighted path lengths in G from a given set of source nodes. all_pairs_dijkstra(G[, cutoff, weight]) Find shortest weighted paths and lengths between all nodes. all_pairs_dijkstra_path(G[, cutoff, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_dijkstra_path_length(G[, cutoff, \u2026]) Compute shortest path lengths between all nodes in a weighted graph. bidirectional_dijkstra(G, source, target[, \u2026]) Dijkstra\u2019s algorithm for shortest paths using bidirectional search. bellman_ford_path(G, source, target[, weight]) Returns the shortest path from source to target in a weighted graph G. bellman_ford_path_length(G, source, target) Returns the shortest path length from source to target in a weighted graph. single_source_bellman_ford(G, source[, \u2026]) Compute shortest paths and lengths in a weighted graph G. single_source_bellman_ford_path(G, source[, \u2026]) Compute shortest path between source and all other reachable nodes for a weighted graph. single_source_bellman_ford_path_length(G, source) Compute the shortest path length between source and all other reachable nodes for a weighted graph. all_pairs_bellman_ford_path(G[, weight]) Compute shortest paths between all nodes in a weighted graph. all_pairs_bellman_ford_path_length(G[, weight]) Compute shortest path lengths between all nodes in a weighted graph. bellman_ford_predecessor_and_distance(G, source) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. negative_edge_cycle(G[, weight]) Returns True if there exists a negative edge cycle anywhere in G. goldberg_radzik(G, source[, weight]) Compute shortest path lengths and predecessors on shortest paths in weighted graphs. johnson(G[, weight]) Uses Johnson\u2019s Algorithm to compute shortest paths. Dense Graphs","title":"Shortest path algorithms for weighed graphs."},{"location":"algorithms/algIII/Algorithms-III/#floyd-warshall-algorithm-for-shortest-paths","text":"floyd_warshall(G[, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_predecessor_and_distance(G[, \u2026]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. floyd_warshall_numpy(G[, nodelist, weight]) Find all-pairs shortest path lengths using Floyd\u2019s algorithm. reconstruct_path(source, target, predecessors) Reconstruct a path from source to target using the predecessors dict as returned by floyd_warshall_predecessor_and_distance","title":"Floyd-Warshall algorithm for shortest paths."},{"location":"algorithms/algIII/Algorithms-III/#a-algorithm","text":"Shortest paths and path lengths using the A* (\u201cA star\u201d) algorithm. astar_path(G, source, target[, heuristic, \u2026]) Returns a list of nodes in a shortest path between source and target using the A* (\u201cA-star\u201d) algorithm. astar_path_length(G, source, target[, \u2026]) Returns the length of the shortest path between source and target using t G = nx . karate_club_graph() nx . draw(G, node_size = 500 , node_color = lightblue , with_labels = True ) nx . shortest_path(G, 0 , 26 ) [0, 8, 33, 26] list (nx . all_shortest_paths(G, 0 , 26 )) [[0, 8, 33, 26], [0, 13, 33, 26], [0, 19, 33, 26], [0, 31, 33, 26]]","title":"A* Algorithm"},{"location":"download/download/","text":"ICD 11 Download Pipeline Setting up API To download ICD11 data you need to use the API provided at: https://icd.who.int/icdapi . In order to gain access to the API you need to create an account and use the client key provided. With the client key you are now able to access all of the endpoints specified in the API documentation . The rest of this guide uses the ICD11 module from the ping lab utils package. You can find and clone the module here: https://github.com/salviStudent/testing/tree/master/testing-master . Working Directory and Additonal Dependencies For the simplest use you need to have a json file named config.json in your working directory. The config file needs to have the following: { ClientId : your_client_id , ClientSecret : your_client_secret } where your_client_id and your_client_secret are your client and secret keys respectively. Along with this config file ICD11.py only needs the request module to function. You can install it by running pip3 install requests if it is not already installed. Getting started Once all of this is in place you are ready to start downloading ICD-11 data. As an example we show the results from the ICD-11 code corresponding to hypertensive heart disease. from ICD11 import icd11_data hypertensive_heart_disease = icd11_data( 1210166201 ) print (hypetensive_heart_disease) This outputs: { @context : http://id.who.int/icd/contexts/contextForFoundationEntity.json , @id : http://id.who.int/icd/entity/1210166201 , parent : [ http://id.who.int/icd/entity/924915526 , http://id.who.int/icd/entity/1395497138 ], child : [ http://id.who.int/icd/entity/600660459 , http://id.who.int/icd/entity/1208029865 ], browserUrl : NA , title : { @language : en , @value : Hypertensive heart disease }, synonym : [ { label : { @language : en , @value : HHD - [hypertensive heart disease] } }, { label : { @language : en , @value : hypertensive cardiac disease } } ], definition : { @language : en , @value : Uncontrolled and prolonged hypertension can lead to a variety of changes in the myocardial structure, coronary vasculature, and conduction system of the heart. Hypertensive heart disease is a term applied generally to heart diseases, such as left ventricular hypertrophy, coronary artery disease, cardiac arrhythmias, and congestive heart failure, that are caused by direct or indirect effects hypertension. } }","title":"Download Pipeline"},{"location":"download/download/#icd-11-download-pipeline","text":"","title":"ICD 11 Download Pipeline"},{"location":"download/download/#setting-up-api","text":"To download ICD11 data you need to use the API provided at: https://icd.who.int/icdapi . In order to gain access to the API you need to create an account and use the client key provided. With the client key you are now able to access all of the endpoints specified in the API documentation . The rest of this guide uses the ICD11 module from the ping lab utils package. You can find and clone the module here: https://github.com/salviStudent/testing/tree/master/testing-master .","title":"Setting up API"},{"location":"download/download/#working-directory-and-additonal-dependencies","text":"For the simplest use you need to have a json file named config.json in your working directory. The config file needs to have the following: { ClientId : your_client_id , ClientSecret : your_client_secret } where your_client_id and your_client_secret are your client and secret keys respectively. Along with this config file ICD11.py only needs the request module to function. You can install it by running pip3 install requests if it is not already installed.","title":"Working Directory and Additonal Dependencies"},{"location":"download/download/#getting-started","text":"Once all of this is in place you are ready to start downloading ICD-11 data. As an example we show the results from the ICD-11 code corresponding to hypertensive heart disease. from ICD11 import icd11_data hypertensive_heart_disease = icd11_data( 1210166201 ) print (hypetensive_heart_disease) This outputs: { @context : http://id.who.int/icd/contexts/contextForFoundationEntity.json , @id : http://id.who.int/icd/entity/1210166201 , parent : [ http://id.who.int/icd/entity/924915526 , http://id.who.int/icd/entity/1395497138 ], child : [ http://id.who.int/icd/entity/600660459 , http://id.who.int/icd/entity/1208029865 ], browserUrl : NA , title : { @language : en , @value : Hypertensive heart disease }, synonym : [ { label : { @language : en , @value : HHD - [hypertensive heart disease] } }, { label : { @language : en , @value : hypertensive cardiac disease } } ], definition : { @language : en , @value : Uncontrolled and prolonged hypertension can lead to a variety of changes in the myocardial structure, coronary vasculature, and conduction system of the heart. Hypertensive heart disease is a term applied generally to heart diseases, such as left ventricular hypertrophy, coronary artery disease, cardiac arrhythmias, and congestive heart failure, that are caused by direct or indirect effects hypertension. } }","title":"Getting started"},{"location":"explore/explore/","text":"","title":"Exploratory"},{"location":"flask/flask/","text":"Implementation of Flask Install flask in current environment pip install flask Import required environment from flask import Flask, request,render_template from flask import g, Response Creating welcome page which renders landing page \"index.html\" Welcome Page @app . route( / ) def welcome (): return render_template( index.html ) Sending data to a route address: @app . route( /boot_ask_one ) def boot (): prepare some data called INFO return render_template( boot_ask_one.html , info = INFO)","title":"Flask"},{"location":"flask/flask/#implementation-of-flask","text":"Install flask in current environment pip install flask Import required environment from flask import Flask, request,render_template from flask import g, Response Creating welcome page which renders landing page \"index.html\" Welcome Page @app . route( / ) def welcome (): return render_template( index.html ) Sending data to a route address: @app . route( /boot_ask_one ) def boot (): prepare some data called INFO return render_template( boot_ask_one.html , info = INFO)","title":"Implementation of Flask"},{"location":"flask/flask_connections/","text":"Connecting flask to neo4j, mongoDB, and elasticSearch Overview Because our application's requirement's involve us using a heterogenous mix of data sources we needed a way to bring all of these data sources together quickly and easily. To do this we chose flask, a python webframework that allows us to efficiently manage the complexity involved with dealing with all three databases. The general problem is this: repeatedly connecting to each database is not only relatively computationally expensive over time but interacting with each database by itself is not necessarily intuitive, as each database has its own somewhat similar but ultimately different query syntax. To work around this we created three distinct python modules that create a consistent connection to each database as a python variable, create high level functionality based on python functions that take these variables as arguments and then finally mix and match these functions to handle various tasks. The general pattern for each module is as follows: database_utils/ __init__.py database_funcs.py constants.py where __init__.py specifies what is available for a program importing database_utils, constants.py has the connection to the specified connection and any other relevant constants, and database_funcs.py has all of the function implmentations. Below is an example of this pattern with part of our neo4j_utils module. neo4j","title":"Connecting flask to neo4j, mongoDB, and elasticSearch"},{"location":"flask/flask_connections/#connecting-flask-to-neo4j-mongodb-and-elasticsearch","text":"","title":"Connecting flask to neo4j, mongoDB, and elasticSearch"},{"location":"flask/flask_connections/#overview","text":"Because our application's requirement's involve us using a heterogenous mix of data sources we needed a way to bring all of these data sources together quickly and easily. To do this we chose flask, a python webframework that allows us to efficiently manage the complexity involved with dealing with all three databases. The general problem is this: repeatedly connecting to each database is not only relatively computationally expensive over time but interacting with each database by itself is not necessarily intuitive, as each database has its own somewhat similar but ultimately different query syntax. To work around this we created three distinct python modules that create a consistent connection to each database as a python variable, create high level functionality based on python functions that take these variables as arguments and then finally mix and match these functions to handle various tasks. The general pattern for each module is as follows: database_utils/ __init__.py database_funcs.py constants.py where __init__.py specifies what is available for a program importing database_utils, constants.py has the connection to the specified connection and any other relevant constants, and database_funcs.py has all of the function implmentations. Below is an example of this pattern with part of our neo4j_utils module.","title":"Overview"},{"location":"flask/flask_connections/#neo4j","text":"","title":"neo4j"},{"location":"graph/graph/","text":"ICD 11 Graph NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] } Implementation of NetworkX Install networkx pip install networkx Import networkx in the current environment import networkx as nx Initiate vacant graph G = nx . Graph() Introduce node id and properties from each item of parsed Data G . add_node(item_id,\\ title = item[ title ],\\ code = item[ code ],\\ defn = item[ defn ],\\ childs = item[ childs ],\\ parents = item[ parents ]) Introduce child relationship as edges of the graph from each item of parsed Data childs = item[ childs ] for c_id in childs: G . add_edge(item_id,c_id) Implementation of Neo4j (EPOCH) Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (n:icd11_code) DETACH DELETE n ) Create nodes in the graph with driver . session() as session: session . run( call apoc.load.json( file:///clean_mms_graph_data.json ) yield value create (node:icd11_code) set node = value.props; ) Create an index on each node's id property, otherwise the next command will take too long with driver . session() as session: session . run( create index on :icd11_code(id) ) Create edge with relationship 'child_of' with driver . session() as session: sesh . run( match (n:icd11_code) with n , n.child_nodes as child_nodes match (m:icd11_code) where m.id in child_nodes with n, m create (n) -[:child_of]-(m) ) Implementation of Neo4j (Individual) Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Graph Database"},{"location":"graph/graph/#icd-11-graph","text":"NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] }","title":"ICD 11 Graph"},{"location":"graph/graph/#implementation-of-networkx","text":"Install networkx pip install networkx Import networkx in the current environment import networkx as nx Initiate vacant graph G = nx . Graph() Introduce node id and properties from each item of parsed Data G . add_node(item_id,\\ title = item[ title ],\\ code = item[ code ],\\ defn = item[ defn ],\\ childs = item[ childs ],\\ parents = item[ parents ]) Introduce child relationship as edges of the graph from each item of parsed Data childs = item[ childs ] for c_id in childs: G . add_edge(item_id,c_id)","title":"Implementation of NetworkX"},{"location":"graph/graph/#implementation-of-neo4j-epoch","text":"Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (n:icd11_code) DETACH DELETE n ) Create nodes in the graph with driver . session() as session: session . run( call apoc.load.json( file:///clean_mms_graph_data.json ) yield value create (node:icd11_code) set node = value.props; ) Create an index on each node's id property, otherwise the next command will take too long with driver . session() as session: session . run( create index on :icd11_code(id) ) Create edge with relationship 'child_of' with driver . session() as session: sesh . run( match (n:icd11_code) with n , n.child_nodes as child_nodes match (m:icd11_code) where m.id in child_nodes with n, m create (n) -[:child_of]-(m) )","title":"Implementation of Neo4j (EPOCH)"},{"location":"graph/graph/#implementation-of-neo4j-individual","text":"Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Implementation of Neo4j (Individual)"},{"location":"graph/query/","text":"Query and Algorithms NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] } Implementation of NetworkX Install networkx pip install networkx Import networkx in the current environment import networkx as nx Initiate vacant graph G = nx . Graph() Introduce node id and properties from each item of parsed Data G . add_node(item_id,\\ title = item[ title ],\\ code = item[ code ],\\ defn = item[ defn ],\\ childs = item[ childs ],\\ parents = item[ parents ]) Introduce child relationship as edges of the graph from each item of parsed Data childs = item[ childs ] for c_id in childs: G . add_edge(item_id,c_id) Implementation of Neo4j Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Query"},{"location":"graph/query/#query-and-algorithms","text":"NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] }","title":"Query and Algorithms"},{"location":"graph/query/#implementation-of-networkx","text":"Install networkx pip install networkx Import networkx in the current environment import networkx as nx Initiate vacant graph G = nx . Graph() Introduce node id and properties from each item of parsed Data G . add_node(item_id,\\ title = item[ title ],\\ code = item[ code ],\\ defn = item[ defn ],\\ childs = item[ childs ],\\ parents = item[ parents ]) Introduce child relationship as edges of the graph from each item of parsed Data childs = item[ childs ] for c_id in childs: G . add_edge(item_id,c_id)","title":"Implementation of NetworkX"},{"location":"graph/query/#implementation-of-neo4j","text":"Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Implementation of Neo4j"},{"location":"graph/query_neo4j/","text":"Query and Algorithms NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] } Implementation of NetworkX Install networkx pip install networkx Import networkx in the current environment import networkx as nx Initiate vacant graph G = nx . Graph() Introduce node id and properties from each item of parsed Data G . add_node(item_id,\\ title = item[ title ],\\ code = item[ code ],\\ defn = item[ defn ],\\ childs = item[ childs ],\\ parents = item[ parents ]) Introduce child relationship as edges of the graph from each item of parsed Data childs = item[ childs ] for c_id in childs: G . add_edge(item_id,c_id) Implementation of Neo4j Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Queries in Neo4j"},{"location":"graph/query_neo4j/#query-and-algorithms","text":"NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] }","title":"Query and Algorithms"},{"location":"graph/query_neo4j/#implementation-of-networkx","text":"Install networkx pip install networkx Import networkx in the current environment import networkx as nx Initiate vacant graph G = nx . Graph() Introduce node id and properties from each item of parsed Data G . add_node(item_id,\\ title = item[ title ],\\ code = item[ code ],\\ defn = item[ defn ],\\ childs = item[ childs ],\\ parents = item[ parents ]) Introduce child relationship as edges of the graph from each item of parsed Data childs = item[ childs ] for c_id in childs: G . add_edge(item_id,c_id)","title":"Implementation of NetworkX"},{"location":"graph/query_neo4j/#implementation-of-neo4j","text":"Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Implementation of Neo4j"},{"location":"graph/query_nx/","text":"Query and Algorithms NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] } Exploring single node These steps are dedicated to explore the single node information Finding root information For the provided IDs of root nodes, we can extrace the root node information ROOT = [n for n in G . neighbors( ICD11 )] INFO = [] for node in ROOT: try : INFO . append({ ID : node,\\ title : G . nodes()[node][ title ],\\ defn : G . nodes()[node][ defn ],\\ child : len ([n for n in G . neighbors(node)])}) except : INFO . append({ ID : node,\\ title : NA ,\\ defn : NA ,\\ child : NA }) Finding Root node For a specific node, one can find the corresponding root node Prepare root info root_of_source_node = nx . shortest_path(G,source = ROOT,target = source_node)[ 1 ] Finding neighborhood For a specific node, neighborhood node information can be collected. Collect Nrighbour info source_neighbours = G . neighbors(source_node) NBD = [] for item in source_neighbours: NBD . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]}) Finding path information For a given node, the path from root to the specific node path information can be collected. Collect Path Info path = nx . shortest_path(G, ROOT, source_node) PATH_NAMES = [] for item in path: PATH_NAMES . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]}) Collect Subgraph For a specific node, the subgraph can be created by extracting neighbourhood of the node. Collect Subgraph info SG = nx . Graph() for node in path: SG . add_node(node,title = G . nodes()[node][ title ],\\ defn = G . nodes()[node][ defn ],\\ code = G . nodes()[node][ code ]) nbd = [n for n in G . neighbors(node)] for item in nbd: try : SG . add_node(item,title = G . nodes()[item][ title ],\\ defn = G . nodes()[item][ defn ],\\ code = G . nodes()[item][ code ]) except : SG . add_node(item,title = Key not found ,\\ defn = key not found ,\\ code = key not found ) for item in nbd: SG . add_edge(node,item) Exploring two node association Following are the methods to explore node to node association Source and target node Collect source and target node information identify source and target source_node = result[ source ] target_node = result[ target ] source_title = G . nodes()[source_node][ title ] target_title = G . nodes()[target_node][ title ] source_defn = G . nodes()[source_node][ defn ] target_defn = G . nodes()[target_node][ defn ] Neighborhood of nodes find neighbourhood info source_neighbours = G . neighbors(source_node) target_neighbours = G . neighbors(target_node) s_NBD = [] for item in source_neighbours: s_NBD . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]}) t_NBD = [] for item in target_neighbours: t_NBD . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]}) Root node information find root info source_root = nx . shortest_path(G,source = ROOT,target = source_node)[ 1 ] target_root = nx . shortest_path(G,source = ROOT,target = target_node)[ 1 ] source_root_title = G . nodes()[source_root][ title ] target_root_title = G . nodes()[target_root][ title ] source_root_defn = G . nodes()[source_root][ defn ] target_root_defn = G . nodes()[target_root][ defn ] Path information Collect path items path = nx . shortest_path(G, source_node, target_node) if ICD11 in path: common_child == False PATH_NAMES = [] for item in path: PATH_NAMES . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]}) Subgraph Information Find subgraph info SG = nx . Graph() for node in path: nbd = [n for n in G . neighbors(node)] SG . add_node(node) for item in nbd: SG . add_node(item) for item in nbd: SG . add_edge(node,item)","title":"Queries in NetworkX"},{"location":"graph/query_nx/#query-and-algorithms","text":"NetworkX and Neo4j has been implemented for creation of ICD 11 graph. The parsed document for each of the node in ICD 11 graph has the following propreties: { id : 496418968 , code : EB13.2 , title : Stevens-Johnson and toxic epidermal necrolysis overlap syndrome , defn : A severe reactive skin disorder with features of both toxic epidermal... , syns : NA , childs : [ 1077457356 ], parents : [ 195467267 ] }","title":"Query and Algorithms"},{"location":"graph/query_nx/#exploring-single-node","text":"These steps are dedicated to explore the single node information","title":"Exploring single node"},{"location":"graph/query_nx/#finding-root-information","text":"For the provided IDs of root nodes, we can extrace the root node information ROOT = [n for n in G . neighbors( ICD11 )] INFO = [] for node in ROOT: try : INFO . append({ ID : node,\\ title : G . nodes()[node][ title ],\\ defn : G . nodes()[node][ defn ],\\ child : len ([n for n in G . neighbors(node)])}) except : INFO . append({ ID : node,\\ title : NA ,\\ defn : NA ,\\ child : NA })","title":"Finding root information"},{"location":"graph/query_nx/#finding-root-node","text":"For a specific node, one can find the corresponding root node Prepare root info root_of_source_node = nx . shortest_path(G,source = ROOT,target = source_node)[ 1 ]","title":"Finding Root node"},{"location":"graph/query_nx/#finding-neighborhood","text":"For a specific node, neighborhood node information can be collected. Collect Nrighbour info source_neighbours = G . neighbors(source_node) NBD = [] for item in source_neighbours: NBD . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]})","title":"Finding neighborhood"},{"location":"graph/query_nx/#finding-path-information","text":"For a given node, the path from root to the specific node path information can be collected. Collect Path Info path = nx . shortest_path(G, ROOT, source_node) PATH_NAMES = [] for item in path: PATH_NAMES . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]})","title":"Finding path information"},{"location":"graph/query_nx/#collect-subgraph","text":"For a specific node, the subgraph can be created by extracting neighbourhood of the node. Collect Subgraph info SG = nx . Graph() for node in path: SG . add_node(node,title = G . nodes()[node][ title ],\\ defn = G . nodes()[node][ defn ],\\ code = G . nodes()[node][ code ]) nbd = [n for n in G . neighbors(node)] for item in nbd: try : SG . add_node(item,title = G . nodes()[item][ title ],\\ defn = G . nodes()[item][ defn ],\\ code = G . nodes()[item][ code ]) except : SG . add_node(item,title = Key not found ,\\ defn = key not found ,\\ code = key not found ) for item in nbd: SG . add_edge(node,item)","title":"Collect Subgraph"},{"location":"graph/query_nx/#exploring-two-node-association","text":"Following are the methods to explore node to node association","title":"Exploring two node association"},{"location":"graph/query_nx/#source-and-target-node","text":"Collect source and target node information identify source and target source_node = result[ source ] target_node = result[ target ] source_title = G . nodes()[source_node][ title ] target_title = G . nodes()[target_node][ title ] source_defn = G . nodes()[source_node][ defn ] target_defn = G . nodes()[target_node][ defn ]","title":"Source and target node"},{"location":"graph/query_nx/#neighborhood-of-nodes","text":"find neighbourhood info source_neighbours = G . neighbors(source_node) target_neighbours = G . neighbors(target_node) s_NBD = [] for item in source_neighbours: s_NBD . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]}) t_NBD = [] for item in target_neighbours: t_NBD . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]})","title":"Neighborhood of nodes"},{"location":"graph/query_nx/#root-node-information","text":"find root info source_root = nx . shortest_path(G,source = ROOT,target = source_node)[ 1 ] target_root = nx . shortest_path(G,source = ROOT,target = target_node)[ 1 ] source_root_title = G . nodes()[source_root][ title ] target_root_title = G . nodes()[target_root][ title ] source_root_defn = G . nodes()[source_root][ defn ] target_root_defn = G . nodes()[target_root][ defn ]","title":"Root node information"},{"location":"graph/query_nx/#path-information","text":"Collect path items path = nx . shortest_path(G, source_node, target_node) if ICD11 in path: common_child == False PATH_NAMES = [] for item in path: PATH_NAMES . append({ id :item,\\ title :G . nodes()[item][ title ],\\ code : G . nodes()[item][ code ]})","title":"Path information"},{"location":"graph/query_nx/#subgraph-information","text":"Find subgraph info SG = nx . Graph() for node in path: nbd = [n for n in G . neighbors(node)] SG . add_node(node) for item in nbd: SG . add_node(item) for item in nbd: SG . add_edge(node,item)","title":"Subgraph Information"},{"location":"indexing/indexing/","text":"Indexing of ICD 11 Documents Indexing of documents is done in two steps Import the required libraries from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q Initialization of Index Prepare Name and number of shards, clusters etc. INDEX_NAME = icd11 NUMBER_SHARDS = 1 NUMBER_REPLICAS = 0 Prepare request body request_body = { settings : { number_of_shards : NUMBER_SHARDS, number_of_replicas : NUMBER_REPLICAS }, mappings : { properties : { id : { type : keyword }, tree :{ type : text }, name :{ type : text }, root :{ type : text }, degree :{ type : integer }, definition :{ type : text }, synonym :{ type : text } } } } Call Elasticsearch es = Elasticsearch() Facilitate deleting old indexing if already exist if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( Deleting index %s , Response: %s % (INDEX_NAME, res)) Create new Indexing res = es . indices . create(index = INDEX_NAME, body = request_body) print ( Create index %s , Response: %s % (INDEX_NAME, res)) Populating the Index For each item in the data list, create the data dictionary and op dictionary create data dictionary data_dict = {} data_dict[ id ] = item[ id ] data_dict[ tree ] = item[ tree ] data_dict[ root ] = item[ root ] data_dict[ name ] = item[ name ] data_dict[ parents ] = item[ parents ] data_dict[ childs ] = item[ childs ] data_dict[ sibls ] = item[ sibls ] data_dict[ degree ] = item[ degree ] data_dict[ synonym ] = item[ synonym ] data_dict[ definition ] = item[ definition ] create op dictionary op_dict = { index : { _index : INDEX_NAME, _id : data_dict[ id ] } } Add each data dictionary into bulk data list Put current data into the bulk bulk_data . append(op_dict) bulk_data . append(data_dict) Push the doc dictionary to Indexing INDEX_NAME = icd11 bulk_size = 50 es = Elasticsearch() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 )","title":"Indexing Database"},{"location":"indexing/indexing/#indexing-of-icd-11-documents","text":"Indexing of documents is done in two steps","title":"Indexing of ICD 11 Documents"},{"location":"indexing/indexing/#import-the-required-libraries","text":"from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q","title":"Import the required libraries"},{"location":"indexing/indexing/#initialization-of-index","text":"Prepare Name and number of shards, clusters etc. INDEX_NAME = icd11 NUMBER_SHARDS = 1 NUMBER_REPLICAS = 0 Prepare request body request_body = { settings : { number_of_shards : NUMBER_SHARDS, number_of_replicas : NUMBER_REPLICAS }, mappings : { properties : { id : { type : keyword }, tree :{ type : text }, name :{ type : text }, root :{ type : text }, degree :{ type : integer }, definition :{ type : text }, synonym :{ type : text } } } } Call Elasticsearch es = Elasticsearch() Facilitate deleting old indexing if already exist if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( Deleting index %s , Response: %s % (INDEX_NAME, res)) Create new Indexing res = es . indices . create(index = INDEX_NAME, body = request_body) print ( Create index %s , Response: %s % (INDEX_NAME, res))","title":"Initialization of Index"},{"location":"indexing/indexing/#populating-the-index","text":"For each item in the data list, create the data dictionary and op dictionary create data dictionary data_dict = {} data_dict[ id ] = item[ id ] data_dict[ tree ] = item[ tree ] data_dict[ root ] = item[ root ] data_dict[ name ] = item[ name ] data_dict[ parents ] = item[ parents ] data_dict[ childs ] = item[ childs ] data_dict[ sibls ] = item[ sibls ] data_dict[ degree ] = item[ degree ] data_dict[ synonym ] = item[ synonym ] data_dict[ definition ] = item[ definition ] create op dictionary op_dict = { index : { _index : INDEX_NAME, _id : data_dict[ id ] } } Add each data dictionary into bulk data list Put current data into the bulk bulk_data . append(op_dict) bulk_data . append(data_dict) Push the doc dictionary to Indexing INDEX_NAME = icd11 bulk_size = 50 es = Elasticsearch() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 )","title":"Populating the Index"},{"location":"models/clustering/","text":"Coming Soon Under construction","title":"Clustering"},{"location":"models/clustering/#coming-soon","text":"Under construction","title":"Coming Soon"},{"location":"models/link/","text":"Coming Soon Under construction","title":"Link Prediction"},{"location":"models/link/#coming-soon","text":"Under construction","title":"Coming Soon"},{"location":"models/random-walk/","text":"Coming Soon Under construction","title":"Randomwalk"},{"location":"models/random-walk/#coming-soon","text":"Under construction","title":"Coming Soon"},{"location":"mongo/mongo/","text":"Database After setting up mongoDB database and once parsed data is ready, mongoDB database can be populated with pared data and mapping table Install pymongo in the current environment pip install pymongo Import pymongo to current environment import pymongo Call client for mongoDB database client = pymongo . MongoClient( mongodb://localhost:27017/ ) Create new database Delete previously created database client . drop_database( icd11 ) Create a new database with new name database = client[ NAME ] Add Collection in the new database Create new collection inside the database collection = database[ COLLECTION_NAME ] Populating collection with parsed data x = collection . insert_many(Data) Test a sample out of populated data x = collection . find_one() print (x) Add mapping Tables as new collections Create seperate collection within the database for seperate mapping table mapping_collection = db[ MAPPING_NAME ] Populate the collection with mapping table x = mapping_collection . insert_many(MAPPING_TABLE)","title":"NoSQL Database"},{"location":"mongo/mongo/#database","text":"After setting up mongoDB database and once parsed data is ready, mongoDB database can be populated with pared data and mapping table Install pymongo in the current environment pip install pymongo Import pymongo to current environment import pymongo Call client for mongoDB database client = pymongo . MongoClient( mongodb://localhost:27017/ )","title":"Database"},{"location":"mongo/mongo/#create-new-database","text":"Delete previously created database client . drop_database( icd11 ) Create a new database with new name database = client[ NAME ]","title":"Create new database"},{"location":"mongo/mongo/#add-collection-in-the-new-database","text":"Create new collection inside the database collection = database[ COLLECTION_NAME ] Populating collection with parsed data x = collection . insert_many(Data) Test a sample out of populated data x = collection . find_one() print (x)","title":"Add Collection in the new database"},{"location":"mongo/mongo/#add-mapping-tables-as-new-collections","text":"Create seperate collection within the database for seperate mapping table mapping_collection = db[ MAPPING_NAME ] Populate the collection with mapping table x = mapping_collection . insert_many(MAPPING_TABLE)","title":"Add mapping Tables as new collections"},{"location":"parsing/parsing/","text":"Parsing and Mapping Parsing Different domponent of the MMS data and creation of mapping Parsing ID idterm = str (item[ @id ] . split( / )[ - 1 ]) if idterm . encode( utf-8 ) in [ other , unspecified ]: ID = str (item[ @id ] . split( / )[ - 2 ]) + / + idterm else : ID = idterm Parsing Code and ID to Code mapping try : code = str (item[ code ]) id2code . update({ID:code}) except : code = NA id2code . update({ID:code}) Parsing Title and ID to title mapping title = item[ title ][ @value ] id to title mapping id2title . update({ID:title}) Parsing Definition try: defn = str(item[ definition ][ @value ]) except: defn = NA Parsing Foundation term try : index_term = item[ indexTerm ] INDEX_TERM = [] for it in index_term: try : index_term_title = str (it[ label ][ @value ]) except : index_term_title = NA try : index_term_foundation_id = str (it[ foundationReference ] . split( / )[ - 1 ]) except : index_term_foundation_id = NA INDEX_TERM . append({ indexTerm_title :index_term_title,\\ indexTerm_foundation_id :index_term_foundation_id}) update mapping table id2index . update({ID:INDEX_TERM}) except : index_term = [] update mapping table id2index . update({ID:INDEX_TERM}) Parsing Childrens try : childs = item[ child ] if len (childs) 0 : CHILDS = [] for c in childs: child_idterm = str (c . split( / )[ - 1 ]) if child_idterm in [ other , unspecified ]: CHILDS . append( str (c . split( / )[ - 2 ]) + / + str (c . split( / )[ - 1 ])) else : CHILDS . append( str (c . split( / )[ - 1 ])) has_children = len (CHILDS) else : CHILDS = [] has_children = 0 is_leaf_node = True except : CHILDS = [] is_leaf_node = True has_children = 0 Parsing Parents try : parents = item[ parent ] if len (parents) 0 : PARENTS = [] for p in parents: PARENTS . append( str (p . split( / )[ - 1 ])) has_parents = len (parents) else : PARENTS = [] has_parents = 0 except : PARENTS = [] has_parents = 0 Parsing Post Coordination Term try : POST_COORDINATION_SCALE = [] postcoordinationScale = item[ postcoordinationScale ] for pcs in postcoordinationScale: try : allowMultipleValues = str (pcs[ allowMultipleValues ]) except : allowMultipleValues = NA try : axisName = str (pcs[ axisName ] . split( / )[ - 1 ]) except : axisName = NA try : requiredPostcoordination = str (pcs[ requiredPostcoordination ]) except : requiredPostcoordination = NA try : scaleEntity = pcs[ scaleEntity ] SCALE_ENTITY = [] if len (scaleEntity) 0 : for e in scaleEntity: idterm = str (e . split( / )[ - 1 ]) if scale_idterm in [ other , unspecified ]: SCALE_ENTITY . append( str (e . split( / )[ - 2 ]) + / + scale_idterm) else : SCALE_ENTITY . append(scale_idterm) except : SCALE_ENTITY = [] POST_COORDINATION_SCALE . append({ allowMultipleValues :allowMultipleValues,\\ axisName :axisName,\\ requiredPostcoordination :requiredPostcoordination,\\ scaleEntity : SCALE_ENTITY}) update mapping table id2pcs . update({ID:POST_COORDINATION_SCALE}) except : POST_COORDINATION_SCALE = [] update mapping table id2pcs . update({ID:POST_COORDINATION_SCALE})","title":"Parsing Pipeline"},{"location":"parsing/parsing/#parsing-and-mapping","text":"Parsing Different domponent of the MMS data and creation of mapping","title":"Parsing and Mapping"},{"location":"parsing/parsing/#parsing-id","text":"idterm = str (item[ @id ] . split( / )[ - 1 ]) if idterm . encode( utf-8 ) in [ other , unspecified ]: ID = str (item[ @id ] . split( / )[ - 2 ]) + / + idterm else : ID = idterm","title":"Parsing ID"},{"location":"parsing/parsing/#parsing-code-and-id-to-code-mapping","text":"try : code = str (item[ code ]) id2code . update({ID:code}) except : code = NA id2code . update({ID:code})","title":"Parsing Code and ID to Code mapping"},{"location":"parsing/parsing/#parsing-title-and-id-to-title-mapping","text":"title = item[ title ][ @value ] id to title mapping id2title . update({ID:title})","title":"Parsing Title and ID to title mapping"},{"location":"parsing/parsing/#parsing-definition","text":"try: defn = str(item[ definition ][ @value ]) except: defn = NA","title":"Parsing Definition"},{"location":"parsing/parsing/#parsing-foundation-term","text":"try : index_term = item[ indexTerm ] INDEX_TERM = [] for it in index_term: try : index_term_title = str (it[ label ][ @value ]) except : index_term_title = NA try : index_term_foundation_id = str (it[ foundationReference ] . split( / )[ - 1 ]) except : index_term_foundation_id = NA INDEX_TERM . append({ indexTerm_title :index_term_title,\\ indexTerm_foundation_id :index_term_foundation_id}) update mapping table id2index . update({ID:INDEX_TERM}) except : index_term = [] update mapping table id2index . update({ID:INDEX_TERM})","title":"Parsing Foundation term"},{"location":"parsing/parsing/#parsing-childrens","text":"try : childs = item[ child ] if len (childs) 0 : CHILDS = [] for c in childs: child_idterm = str (c . split( / )[ - 1 ]) if child_idterm in [ other , unspecified ]: CHILDS . append( str (c . split( / )[ - 2 ]) + / + str (c . split( / )[ - 1 ])) else : CHILDS . append( str (c . split( / )[ - 1 ])) has_children = len (CHILDS) else : CHILDS = [] has_children = 0 is_leaf_node = True except : CHILDS = [] is_leaf_node = True has_children = 0","title":"Parsing Childrens"},{"location":"parsing/parsing/#parsing-parents","text":"try : parents = item[ parent ] if len (parents) 0 : PARENTS = [] for p in parents: PARENTS . append( str (p . split( / )[ - 1 ])) has_parents = len (parents) else : PARENTS = [] has_parents = 0 except : PARENTS = [] has_parents = 0","title":"Parsing Parents"},{"location":"parsing/parsing/#parsing-post-coordination-term","text":"try : POST_COORDINATION_SCALE = [] postcoordinationScale = item[ postcoordinationScale ] for pcs in postcoordinationScale: try : allowMultipleValues = str (pcs[ allowMultipleValues ]) except : allowMultipleValues = NA try : axisName = str (pcs[ axisName ] . split( / )[ - 1 ]) except : axisName = NA try : requiredPostcoordination = str (pcs[ requiredPostcoordination ]) except : requiredPostcoordination = NA try : scaleEntity = pcs[ scaleEntity ] SCALE_ENTITY = [] if len (scaleEntity) 0 : for e in scaleEntity: idterm = str (e . split( / )[ - 1 ]) if scale_idterm in [ other , unspecified ]: SCALE_ENTITY . append( str (e . split( / )[ - 2 ]) + / + scale_idterm) else : SCALE_ENTITY . append(scale_idterm) except : SCALE_ENTITY = [] POST_COORDINATION_SCALE . append({ allowMultipleValues :allowMultipleValues,\\ axisName :axisName,\\ requiredPostcoordination :requiredPostcoordination,\\ scaleEntity : SCALE_ENTITY}) update mapping table id2pcs . update({ID:POST_COORDINATION_SCALE}) except : POST_COORDINATION_SCALE = [] update mapping table id2pcs . update({ID:POST_COORDINATION_SCALE})","title":"Parsing Post Coordination Term"},{"location":"setup/anaconda/","text":"Installing Python To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python. Note- Linux: For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook Note - Cloud For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Setting up Python"},{"location":"setup/anaconda/#installing-python","text":"To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python.","title":"Installing Python"},{"location":"setup/anaconda/#note-linux","text":"For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook","title":"Note- Linux:"},{"location":"setup/anaconda/#note-cloud","text":"For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Note - Cloud"},{"location":"setup/apache_setup/","text":"Setting up apache2 Apache2 is the world's most used webserver and the webserver that we are currently using to serve our application on the internet. More information on apache can be found here . Downloading and Installing apache2 apache2 normally is shipped with most *nix based machines(macOS, linux distros) however if it is not installed for any reason you can follow the instructions here for your specific machine. The rest of the docs assume you are running apache2 on some variant of Debian, in our case ubuntu. Connecting apache2 to flask Once apache 2 is installed we have to actually connect it to our flask instance. To do this we can use the following file as our configuration file. VirtualHost *:80 WSGIDaemonProcess app_name python-home=path/to/python/version/directory WSGIProcessGroup app_name WSGIScriptAlias / /path/to/wsgi/file Directory /path/to/flask/project/directory Order allow,deny Allow from all /Directory ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined /VirtualHost What this file does is specify where apache2 can find the python interpreter for our desired application, name the threads that run under this application, and set where the access and error logs will be placed. You need to copy the above code and put it in a file 'your_app_name.conf' and move it to the following location: /etc/apache2/sites-available. Once the file is in this directory you execute the following command: sudo a2ensite your_app_name this command tell apache to run the conf file for your application. If there were no errors you should be able to navigate to your machines IP address and your site will be displayed in your browser.","title":"Setting up apache2"},{"location":"setup/apache_setup/#setting-up-apache2","text":"Apache2 is the world's most used webserver and the webserver that we are currently using to serve our application on the internet. More information on apache can be found here .","title":"Setting up apache2"},{"location":"setup/apache_setup/#downloading-and-installing-apache2","text":"apache2 normally is shipped with most *nix based machines(macOS, linux distros) however if it is not installed for any reason you can follow the instructions here for your specific machine. The rest of the docs assume you are running apache2 on some variant of Debian, in our case ubuntu.","title":"Downloading and Installing apache2"},{"location":"setup/apache_setup/#connecting-apache2-to-flask","text":"Once apache 2 is installed we have to actually connect it to our flask instance. To do this we can use the following file as our configuration file. VirtualHost *:80 WSGIDaemonProcess app_name python-home=path/to/python/version/directory WSGIProcessGroup app_name WSGIScriptAlias / /path/to/wsgi/file Directory /path/to/flask/project/directory Order allow,deny Allow from all /Directory ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined /VirtualHost What this file does is specify where apache2 can find the python interpreter for our desired application, name the threads that run under this application, and set where the access and error logs will be placed. You need to copy the above code and put it in a file 'your_app_name.conf' and move it to the following location: /etc/apache2/sites-available. Once the file is in this directory you execute the following command: sudo a2ensite your_app_name this command tell apache to run the conf file for your application. If there were no errors you should be able to navigate to your machines IP address and your site will be displayed in your browser.","title":"Connecting apache2 to flask"},{"location":"setup/elastic/","text":"Setting UP ElasticSearch Elasticsearch provides the functionality for Indexing and Search of the text documents. More information can be obtained from official website for Elasticsearch . Downloading and Installing Make sure that you have proper version of Java installed in your device Download extract the elasticsearch and kibana from Elasticsearch website Go to the bin folder and run ./elasticsearch Elasticsearch functionality can be used from Python environment which need to use package elasticsearch and elasticsearch_dsl Create the indexing of the documents Run kibana by running ./kibana from bin folder of extracted kibana. If index is successfully created, the new index can be seen at `localhost/IP:5601``` To run kibana reotely setup 0.0.0.0 in the configuration file. Sample Application with Python Install packages in environment pip install elasticsearch pip install elasticsearch_dsl Setting up Indexing request_body = { settings : { number_of_shards : 1 , number_of_replicas : 0 }, mappings : { pubmed: { properties : { pmid : { type : keyword }, mesh_heading : { type : text , similarity : BM25 }, abstract :{ type : text } } } } } es = Elasticsearch() res = es . indices . create(index = pubmed, body = request_body) Setup search entity = insulin s = Search(using = es, index = pubmed )\\ . params(request_timeout = 300 )\\ . query( match_phrase , abstract = entity) for hit in s . scan(): print (hit . abstract) Read more Elasticsearch Documentation","title":"Setting up Elasticsearch"},{"location":"setup/elastic/#setting-up-elasticsearch","text":"Elasticsearch provides the functionality for Indexing and Search of the text documents. More information can be obtained from official website for Elasticsearch .","title":"Setting UP ElasticSearch"},{"location":"setup/elastic/#downloading-and-installing","text":"Make sure that you have proper version of Java installed in your device Download extract the elasticsearch and kibana from Elasticsearch website Go to the bin folder and run ./elasticsearch Elasticsearch functionality can be used from Python environment which need to use package elasticsearch and elasticsearch_dsl Create the indexing of the documents Run kibana by running ./kibana from bin folder of extracted kibana. If index is successfully created, the new index can be seen at `localhost/IP:5601``` To run kibana reotely setup 0.0.0.0 in the configuration file.","title":"Downloading and Installing"},{"location":"setup/elastic/#sample-application-with-python","text":"Install packages in environment pip install elasticsearch pip install elasticsearch_dsl Setting up Indexing request_body = { settings : { number_of_shards : 1 , number_of_replicas : 0 }, mappings : { pubmed: { properties : { pmid : { type : keyword }, mesh_heading : { type : text , similarity : BM25 }, abstract :{ type : text } } } } } es = Elasticsearch() res = es . indices . create(index = pubmed, body = request_body) Setup search entity = insulin s = Search(using = es, index = pubmed )\\ . params(request_timeout = 300 )\\ . query( match_phrase , abstract = entity) for hit in s . scan(): print (hit . abstract)","title":"Sample Application with Python"},{"location":"setup/elastic/#read-more","text":"Elasticsearch Documentation","title":"Read more"},{"location":"setup/env/","text":"Python Environment Basics To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = /Users/username/anaconda/bin: $PATH or update above command line to your .zsh_config file. Install Package :Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Remove Package : Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name Update Package : If you want to update all packages in an environment, which is often useful, use conda update --all List Package : to list installed packages, it's conda list Search Package Name : If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup Environments Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Name : Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy Version : When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 Different Python versions : I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 Activate Environment : for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate Saving and loading environments Export the environment : A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, environment.yaml Create environment from exported file : writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml . Listing environments List the environments : If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root . Removing environments If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ). Using environments One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican . Sharing environments When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda. More to learn To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"Setting up Environment"},{"location":"setup/env/#python-environment","text":"","title":"Python Environment"},{"location":"setup/env/#basics","text":"To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = /Users/username/anaconda/bin: $PATH or update above command line to your .zsh_config file. Install Package :Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Remove Package : Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name Update Package : If you want to update all packages in an environment, which is often useful, use conda update --all List Package : to list installed packages, it's conda list Search Package Name : If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup","title":"Basics"},{"location":"setup/env/#environments","text":"Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Name : Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy Version : When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 Different Python versions : I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 Activate Environment : for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate","title":"Environments"},{"location":"setup/env/#saving-and-loading-environments","text":"Export the environment : A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, environment.yaml Create environment from exported file : writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml .","title":"Saving and loading environments"},{"location":"setup/env/#listing-environments","text":"List the environments : If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root .","title":"Listing environments"},{"location":"setup/env/#removing-environments","text":"If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ).","title":"Removing environments"},{"location":"setup/env/#using-environments","text":"One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican .","title":"Using environments"},{"location":"setup/env/#sharing-environments","text":"When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda.","title":"Sharing environments"},{"location":"setup/env/#more-to-learn","text":"To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"More to learn"},{"location":"setup/flask_connections/","text":"Connecting flask to neo4j, mongoDB, and elasticSearch Overview Because our application's requirement's involve us using a heterogenous mix of data sources we needed a way to bring all of these data sources together quickly and easily. To do this we chose flask, a python webframework that allows us to efficiently manage the complexity involved with dealing with all three databases. The general problem is this: repeatedly connecting to each database is not only relatively computationally expensive over time but interacting with each database by itself is not necessarily intuitive, as each database has its own somewhat similar but ultimately different query syntax. To work around this we created three distinct python modules that create a consistent connection to each database as a python variable, create high level functionality based on python functions that take these variables as arguments and then finally mix and match these functions to handle various tasks. The general pattern for each module is as follows: database_utils/ __init__.py database_funcs.py constants.py where __init__.py specifies what is available for a program importing database_utils, constants.py has the connection to the specified database and any other relevant constants, and database_funcs.py has all of the function implmentations. Below is an example of this pattern with part of our neo4j_utils module. neo4j The following is the structure of our neo4j module: neo4j_utils/ __init__.py neo4j_funcs.py constants.py Below are code snippets from each file with an accompanying explanation of each snippet. __init__.py from .graph_funcs import mms_neighborhood from .constants import NEO4J_DRIVER This file specifies that any program importing neo4j_utils has access to a connection to our neo4j database named NEO4J_DRIVER and a function mms_neighborhood, a function that returns the neighborhood of an ICD11 MMS code up to a distance specified. graph_funcs.py from .constants import NEIGHBORHOOD_QUERY def mms_neighborhood (mms_id, distance, driver): Returns all of the node ids at or below the distance specified. query = NEIGHBORHOOD_QUERY . format(mms_id = mms_id, distance = distance) with driver . session() as sesh: results = sesh . run(query) ids = [result[ adj_node ][ id ] for result in results] return ids This file is a python program that imports a python object named NEIGHBORHOOD_QUERY from the constants.py file and implements a function named mms_neighborhood. mms_neighborhood takes three arguments: an mms_id(as a string), distance (as an integer or suitable numeric type), and a connection to a neo4j database. With the specified mms_id and distance we create a query string that our connection can then send and run. constants.py #All of the constants used for Neo4j related functions from neo4j import GraphDatabase NEO4J_DRIVER = GraphDatabase . driver(uri = bolt://localhost:7687 , auth = ( neo4j , icd11 )) NEIGHBORHOOD_QUERY = MATCH (node:icd11_code)-[:child_of*0.. {distance} ]-(adj_node:icd11_code) WHERE node.id = {mms_id} RETURN adj_node ORDER BY adj_node.height This file is a python program that creates a neo4j connection and the query for finding a neighborhood of a node.","title":"Connecting flask to neo4j, mongoDB, and elasticSearch"},{"location":"setup/flask_connections/#connecting-flask-to-neo4j-mongodb-and-elasticsearch","text":"","title":"Connecting flask to neo4j, mongoDB, and elasticSearch"},{"location":"setup/flask_connections/#overview","text":"Because our application's requirement's involve us using a heterogenous mix of data sources we needed a way to bring all of these data sources together quickly and easily. To do this we chose flask, a python webframework that allows us to efficiently manage the complexity involved with dealing with all three databases. The general problem is this: repeatedly connecting to each database is not only relatively computationally expensive over time but interacting with each database by itself is not necessarily intuitive, as each database has its own somewhat similar but ultimately different query syntax. To work around this we created three distinct python modules that create a consistent connection to each database as a python variable, create high level functionality based on python functions that take these variables as arguments and then finally mix and match these functions to handle various tasks. The general pattern for each module is as follows: database_utils/ __init__.py database_funcs.py constants.py where __init__.py specifies what is available for a program importing database_utils, constants.py has the connection to the specified database and any other relevant constants, and database_funcs.py has all of the function implmentations. Below is an example of this pattern with part of our neo4j_utils module.","title":"Overview"},{"location":"setup/flask_connections/#neo4j","text":"The following is the structure of our neo4j module: neo4j_utils/ __init__.py neo4j_funcs.py constants.py Below are code snippets from each file with an accompanying explanation of each snippet. __init__.py from .graph_funcs import mms_neighborhood from .constants import NEO4J_DRIVER This file specifies that any program importing neo4j_utils has access to a connection to our neo4j database named NEO4J_DRIVER and a function mms_neighborhood, a function that returns the neighborhood of an ICD11 MMS code up to a distance specified. graph_funcs.py from .constants import NEIGHBORHOOD_QUERY def mms_neighborhood (mms_id, distance, driver): Returns all of the node ids at or below the distance specified. query = NEIGHBORHOOD_QUERY . format(mms_id = mms_id, distance = distance) with driver . session() as sesh: results = sesh . run(query) ids = [result[ adj_node ][ id ] for result in results] return ids This file is a python program that imports a python object named NEIGHBORHOOD_QUERY from the constants.py file and implements a function named mms_neighborhood. mms_neighborhood takes three arguments: an mms_id(as a string), distance (as an integer or suitable numeric type), and a connection to a neo4j database. With the specified mms_id and distance we create a query string that our connection can then send and run. constants.py #All of the constants used for Neo4j related functions from neo4j import GraphDatabase NEO4J_DRIVER = GraphDatabase . driver(uri = bolt://localhost:7687 , auth = ( neo4j , icd11 )) NEIGHBORHOOD_QUERY = MATCH (node:icd11_code)-[:child_of*0.. {distance} ]-(adj_node:icd11_code) WHERE node.id = {mms_id} RETURN adj_node ORDER BY adj_node.height This file is a python program that creates a neo4j connection and the query for finding a neighborhood of a node.","title":"neo4j"},{"location":"setup/git/","text":"How to git Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m First commit Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"Setting up Git"},{"location":"setup/git/#how-to-git","text":"Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m First commit Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"How to git"},{"location":"setup/lib/","text":"Python Libraries Following are the best Python Libraries: TensorFlow Scikit-Learn Numpy Keras PyTorch LightGBM Eli5 SciPy Theano Pandas","title":"Python Libraries"},{"location":"setup/lib/#python-libraries","text":"Following are the best Python Libraries: TensorFlow Scikit-Learn Numpy Keras PyTorch LightGBM Eli5 SciPy Theano Pandas","title":"Python Libraries"},{"location":"setup/mongo/","text":"Setting Up MongoDB MongoDB is NOSQL database. THe detail of the installation can be founs at official website of MongoDB . Following is th installation steps obtained at 2019-Oct 15. Downloading and Installing 1. Import the public key used by the package management system. From a terminal, issue the following command to import the MongoDB public GPG Key from https://www.mongodb.org/static/pgp/server-4.2.asc: wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add - The operation should respond with an OK. 2. Create a list file for MongoDB. Create the list file /etc/apt/sources.list.d/mongodb-org-4.2.list for your version of Ubuntu. Click on the appropriate tab for your version of Ubuntu. If you are unsure of what Ubuntu version the host is running, open a terminal or shell on the host and execute lsb_release -dc . Ubuntu 18.04 (Bionic): The following instruction is for Ubuntu 18.04 (Bionic). For Ubuntu 16.04 (Xenial), click on the appropriate tab. Create the /etc/apt/sources.list.d/mongodb-org-4.2.list file for Ubuntu 18.04 (Bionic): echo deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.list 3. Reload local package database. Issue the following command to reload the local package database: sudo apt-get update 4. Install the MongoDB packages. You can install either the latest stable version of MongoDB or a specific version of MongoDB. Install the latest version of MongoDB. Install a specific release of MongoDB. To install the latest stable version, issue the following sudo apt-get install -y mongodb-org Sample application with Python: Install the python package pymongo in the environment pip install pymongo Create Database and Collection import pymongo client = pymongo . MongoClient( mongodb://localhost:27017/ ) db = client[ test ] collection = db[ test_collection ] Next, Populate the collection x = collection . insert_many(DATA) Print out one sample data: x = collection . find_one() print (x) Read More MongoDB Documents MongoDB Tutorials -I MongoDB Tutorials Point","title":"Setting up MongoDB"},{"location":"setup/mongo/#setting-up-mongodb","text":"MongoDB is NOSQL database. THe detail of the installation can be founs at official website of MongoDB . Following is th installation steps obtained at 2019-Oct 15.","title":"Setting Up MongoDB"},{"location":"setup/mongo/#downloading-and-installing","text":"1. Import the public key used by the package management system. From a terminal, issue the following command to import the MongoDB public GPG Key from https://www.mongodb.org/static/pgp/server-4.2.asc: wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add - The operation should respond with an OK. 2. Create a list file for MongoDB. Create the list file /etc/apt/sources.list.d/mongodb-org-4.2.list for your version of Ubuntu. Click on the appropriate tab for your version of Ubuntu. If you are unsure of what Ubuntu version the host is running, open a terminal or shell on the host and execute lsb_release -dc . Ubuntu 18.04 (Bionic): The following instruction is for Ubuntu 18.04 (Bionic). For Ubuntu 16.04 (Xenial), click on the appropriate tab. Create the /etc/apt/sources.list.d/mongodb-org-4.2.list file for Ubuntu 18.04 (Bionic): echo deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.list 3. Reload local package database. Issue the following command to reload the local package database: sudo apt-get update 4. Install the MongoDB packages. You can install either the latest stable version of MongoDB or a specific version of MongoDB. Install the latest version of MongoDB. Install a specific release of MongoDB. To install the latest stable version, issue the following sudo apt-get install -y mongodb-org","title":"Downloading and Installing"},{"location":"setup/mongo/#sample-application-with-python","text":"Install the python package pymongo in the environment pip install pymongo Create Database and Collection import pymongo client = pymongo . MongoClient( mongodb://localhost:27017/ ) db = client[ test ] collection = db[ test_collection ] Next, Populate the collection x = collection . insert_many(DATA) Print out one sample data: x = collection . find_one() print (x)","title":"Sample application with Python:"},{"location":"setup/mongo/#read-more","text":"MongoDB Documents MongoDB Tutorials -I MongoDB Tutorials Point","title":"Read More"},{"location":"setup/neo4j/","text":"Setting Up Neo4j The detail instruction for installing Neo4j(latest version) can be found at official website of Neo4j . Here are the simple steps created the current time (2019 -Oct 15). Before you install, make sure the correct version of java is installed in your device Install Neo4j To install Neo4j Community Edition: sudo apt-get install neo4j = version To install Neo4j Enterprise Edition: sudo apt-get install neo4j-enterprise = version Startting Stopping or Restarting Service To start the neo4j service : sudo service neo4j start Go to IP/localhost:7474 to have neo4j browser. To stop the neo4j service : sudo service neo4j start To restart the neo4j service : sudo service neo4j start For Forgotten Password Stop the neo4j service remove the auth file inside /var/lib/neo4j/data/dbms restart the service Follow server connect command to recreate password For remote access Stop the neo4j service Setup 0.0.0.0 instead of localhost in the configuration file Start the neo4j service How to use new graph database Stop the neo4j service Find out neo4j.config file and repoint the new graph database. Start the neo4j service","title":"Setting up Neo4J"},{"location":"setup/neo4j/#setting-up-neo4j","text":"The detail instruction for installing Neo4j(latest version) can be found at official website of Neo4j . Here are the simple steps created the current time (2019 -Oct 15). Before you install, make sure the correct version of java is installed in your device","title":"Setting Up Neo4j"},{"location":"setup/neo4j/#install-neo4j","text":"To install Neo4j Community Edition: sudo apt-get install neo4j = version To install Neo4j Enterprise Edition: sudo apt-get install neo4j-enterprise = version","title":"Install Neo4j"},{"location":"setup/neo4j/#startting-stopping-or-restarting-service","text":"To start the neo4j service : sudo service neo4j start Go to IP/localhost:7474 to have neo4j browser. To stop the neo4j service : sudo service neo4j start To restart the neo4j service : sudo service neo4j start","title":"Startting Stopping or Restarting Service"},{"location":"setup/neo4j/#for-forgotten-password","text":"Stop the neo4j service remove the auth file inside /var/lib/neo4j/data/dbms restart the service Follow server connect command to recreate password","title":"For Forgotten Password"},{"location":"setup/neo4j/#for-remote-access","text":"Stop the neo4j service Setup 0.0.0.0 instead of localhost in the configuration file Start the neo4j service","title":"For remote access"},{"location":"setup/neo4j/#how-to-use-new-graph-database","text":"Stop the neo4j service Find out neo4j.config file and repoint the new graph database. Start the neo4j service","title":"How to use new graph database"}]}